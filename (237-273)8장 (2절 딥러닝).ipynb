{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2b4c60-412d-4794-981c-8d91d0768c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3578c9dc-6669-4327-8517-a7da7e0fa5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8722c0c0-dd18-4d7e-bd32-0eb02ef90392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandoc 3.5\n",
      "Features: +server +lua\n",
      "Scripting engine: Lua 5.4\n",
      "User data directory: C:\\Users\\ubion\\AppData\\Roaming\\pandoc\n",
      "Copyright (C) 2006-2024 John MacFarlane. Web: https://pandoc.org\n",
      "This is free software; see the source for copying conditions. There is no\n",
      "warranty, not even for merchantability or fitness for a particular purpose.\n"
     ]
    }
   ],
   "source": [
    "!pandoc --version"
   ]
  },
  {
   "cell_type": "raw",
   "id": "697ec39b-0e8c-49b3-9180-976b13f543cf",
   "metadata": {},
   "source": [
    "#237"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc16334d-0c70-4f02-aee9-6e7447c6f5cb",
   "metadata": {},
   "source": [
    "# 2절 딥러닝으로 AI 모델링하기\n",
    "인공지능의 급속한 발전을 가능하게 한 것은 컴퓨터 성능의 발전, 딥러닝 알고리즘과 심층 신경망 구조의 혁신, 그리고 빅데이터의 축적입니다. <br>\n",
    "딥러닝은 인간의 뇌 신경망이 학습하는 방식에서 영감을 얻어 고안되었습니다. 두뇌의 신경세포가 연결된 형태를 모방하여 인공신경망 모델을 만들고 인공신경망을 학습(Learning) 모델로 사용하는 머신러닝 패러다임이 딥러닝(Deep Learning)입니다. <br>\n",
    "여러 층으로 이루어진 신경망 구조와 다양한 딥러닝 알고리즘은 그동안 많은 연구와 혁신의 결과물입니다. 현재 딥러닝은 이미지 인식, 음성 인식, 언어 번역과 같은 자연어 처리 분야에서 높은 성능을 나타내고 있으며, 언어 번역, 고객 상담, 자율주행, 질병 진단 등 딥러닝의 활용 분야가 지속적으로 확장되고 있습니다. <br>\n",
    "이번 섹션에서는 기본적인 인공신경망 개념과 여러 층으로 이루어진 심층신경망을 효과적으로 학습하는 알고리즘을 소개하고, 분류 문제를 위한 간단한 심층신경망을 알아봅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c4ec89-0646-459d-89aa-004396033e5e",
   "metadata": {},
   "source": [
    "## 1.인공신경망\n",
    "두뇌의 신경망 정보처리 구조를 모방하여 만든 컴퓨터 게산 알고리즘이 인공신경망(Arificial Neural Network, ANN)입니다. 두뇌의 신경망은 수많은 뉴런(neuron, 신경세포)이 연결되어 정보를 처리하고 전달합니다. <br>\n",
    "생물학적 뉴런에는 외부 자극(신호)을 수용하는 여러 개의 수상돌기가 있으며, 세포핵에서 이 자극들을 새로운 자극으로 가공합니다. 새로운 자극은 축삭돌기를 통해 전달됩니다. <br>\n",
    "생물학적 뉴런을 인공 뉴런으로 모델링하면, 입력 $x_1$, $x_2$, $x_3$ ⋯ $x_n$에 가중치 $w_1$, $w_2$, $w_3$ ⋯ $w_n$를 곱하고, 이를 모두 더해서 활성화 함수를 통과시키면 출력됩니다. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "일반적으로 인공 뉴런을 노드(node) 또는 유닛(unit)이라고 하며, 인공신경망은 노드들의 그룹으로 연결되어 뇌의 신경망 구조와 유사합니다. 인공신경망에서 원 모양은 노드를 나타내고 화살표는 하나의 노드 출력에서 다른 노드로 입력을 나타냅니다. 인공신경망의 입력과 출력 중간에 있는 층을 은닉층(hidden layer)이라고 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2eb04d-8822-4873-9ff1-93a53af03937",
   "metadata": {},
   "source": [
    "#### 1) 가중치와 편향 이해하기\n",
    "인공신경망 모델을 수학적으로 표현하면 다음 그림과 같습니다. 뉴런에 2개의 입력값 , 이 입력되고, 입력 개수만큼 2개의 가중치(weight) $w_1$, $w_2$와 1개의 편향(bias, 바이어스) $b$를 가지고 있습니다. \n",
    "\n",
    "가중치는 입력 데이터에서 원하는 연산 결과를 만들도록 비중을 부여하는 값이고, 편향은 뉴런의 활성화를 조절하는 상수입니다. 가중치와 편향은 뉴런의 동작 특성을 나타내는 중요한 파라미터로서 이 값들을 조정해 원하는 출력값을 만들어 냅니다. <br>\n",
    "인공신경망으로 실제 정답에 근사하는 출력값을 만들기 위해 뉴런의 가중치와 편향을 반복적으로 조정하며, 이러한 반복적인 과정을 학습이라고 합니다. <p>\n",
    "인공신경망의 가중합을 구하는 수식은 간단한 1차 함수로 표기할 수 있으며, 입력 에 가중치 를 각각 곱하고, 그 값을 모두 더한 후에 추가적으로 편향을 더합니다. \n",
    "\n",
    "$$가중합=\\sum{}_{i}w_1x_1+b=(w_1x_1+w_2x_2)+b$$\n",
    "$$(W : 가중치, b : 편향)$$\n",
    "\n",
    "\n",
    "입력 $x_1$이 매우 큰 값이라면 $x_1$이 연산 결과에 미치는 영향도가 절대적일 것입니다. 하지만 입력 $x_1$에 곱하는 가중치 $w_1$이 0.0001처럼 작은 값이라면, $w_1$×$x_1$의 연산 결과는 0에 가까운 값이 됩니다. 즉, 가중치는 입력 데이터의 연산 결과에 미치는 영향도를 조절하는 요소이고, 편향값 $b$도 가중합을 조정하는 파라미터입니다. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba559220-eebb-40b1-912c-79390c9963d8",
   "metadata": {},
   "source": [
    "#240"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a93e7ba-9fbd-4dfd-a9f2-194a6fb80a54",
   "metadata": {},
   "source": [
    "### 2) 활성화 함수 알아보기\n",
    "생물학적 뉴런은 입력된 자극(신호)이 특정 강도 이상일 때만 다음 뉴런으로 신호를 전달하는데, 인공신경망의 뉴런에서도 동일한 역할을 하는 활성화 함수(activation function)가 있습니다.<br>\n",
    "활성화 함수는 입력값들의 수학적 선형 결합인 가중합 값을 입력받고 일정 기준에 따라 변환하여 값을 출력하는 비선형(또는 선형) 함수입니다. <br>\n",
    "주요 활성화 함수는 시그모이드(sigmoid), 하이퍼볼릭 탄젠트(hyperbolic tangent, tanh), 렐루(ReLU)가 있으며, 최근 인공신경망의 은닉층에서는 렐루를 많이 사용합니다. \n",
    "\n",
    "#### (1) 시그모이드\n",
    "시그모이드 함수의 수식은 다음과 같습니다. \n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}} \\qquad f'(x) = f(x) \\cdot (1 - f(x))\n",
    "$$\n",
    "\n",
    "활성화 함수 $f(x)$와 활성화 함수의 미분 결과 $f'(x)$를 그래프로 나타내면 다음과 같습니다. <br>\n",
    "시그모이드 함수는 입력값을 비선형 형태로 0~1 사이의 값으로 변화시키므로, 로지스틱 회귀와 값은 이진 분류 모델의 결과를 확률적으로 나타내는 데 사용합니다. \n",
    "\n",
    "\n",
    "시그모이드 미분 함수 $f'(x)$의 작은 입력값이 0일 때 0.25에 불과하고 입력 $x$가 매우 작아지거나 커지면 기울기(미분값)가 0에 수렴하는 것을 알 수 있습니다. 인공신경망을 사용하는 딥러닝은 모델학습에 활성화 함수의 기울기(gradient, 그래디언트)를 계속 곱하는 연산이 있으며, 미분값이 작은 시그모이드 함수를 은닉층의 활성화 함수로 사용하면 기울기 소실(vanishing gradient) 문제가 발생해 모델 학습이 제대로 이루어지지 않습니다. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "83122f22-e940-4f76-be95-4979926d8acb",
   "metadata": {},
   "source": [
    "#240"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b57fdb3-cdb3-48da-bfac-3402e3ae3e2a",
   "metadata": {},
   "source": [
    "#### (2) 하이퍼볼릭 탄젠트\n",
    "시그모이드 활성화 함수를 사용하는 경우 은닉층이 많아질수록 효과적으로 학습되지 않는 한계점을 개선하기 위한 방법 중 하나로 하이퍼볼릭 탄젠트 함수가 제안되었습니다. \n",
    "하이퍼볼릭 탄젠트 함수의 출력은 –1과 1 사이의 값이고, 미분값이 시그모이드 함수에 비해 커졌습니다. 하지만 하이퍼볼릭 탄젠트 함수도 $x$값이 크거나 작아짐에 따라 기울기가 매우 작아져서 기울기 소실 문제가 발생합니다.\n",
    "\n",
    "$$\n",
    "f(x) = \\tanh(x) = \\frac{sinhx}{coshx} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "$$\n",
    "e : 지수함수(exponential function)$$\n",
    "\n",
    "\n",
    "\n",
    "#### (3) 렐루\n",
    "렐루 함수는 시그모이드와 하이퍼볼릭 탄젠트 활성화 함수가 갖는 기울기 소실 문제를 해결하기 위한 함수로 현재 가장 많이 사용되는 활성화 함수 중 하나입니다. \n",
    "렐루 함수는 입력값이 양수일 경우 출력값은 입력값 $x$와 같고, 입력값이 음수일 경우 출력값은 0으로 렐루 함수의 수식은 다음과 같습니다. \n",
    "\n",
    "$$f(x) = \\max(0, x) \\quad f(x) = \\begin{cases} 0 & \\text{if } x < 0 \\\\ x & \\text{if } x \\geq x \\end{cases} \\quad f'(x) = \\begin{cases} 0 & \\text{if } x < 0 \\\\ 1 & \\text{if } x \\geq x \\end{cases}$$<p>\n",
    "\n",
    "\n",
    "그래프를 보면 입력값이 양수일 경우 미분값은 항상 1이며, 기울기 소실 문제를 해결할 수 있습니다. 그리고 렐루 함수에는 특별한 연산이 없으므로 연산 속도가 빠릅니다. \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5777f2c9-9e4a-4a9c-9c9e-67871ef0d0ec",
   "metadata": {},
   "source": [
    "#242"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7052f04-51c3-47cc-b8e4-1313c68ff4c9",
   "metadata": {},
   "source": [
    "렐루 함수에도 단점이 있는데, 렐루 함수의 입력값이 음수일 경우 미분값이 항상 0이므로, 입력값이 음수인 뉴런의 출력을 회생시키지 못하는 한계가 있습니다. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### (4) 리키 렐루\n",
    "렐루 함수의 단점을 보완하기 위해 다양하게 변형된 렐루 함수가 제안되었습니다. 그중 리키 렐루는 입력값이 음수일 때 출력값을 0이 아닌 0.001과 같은 매우 작은 값을 출력하는 함수로 수식은 다음과 같습니다.\n",
    "$$f(x) = max(ax,x)$$\n",
    "$a$는 0.01, 0.001과 같이 작은 값으로 설정하는 파라미터입니다. 리키 렐루 함수는 입력값 0 이하에서도 기울기가 0이 되지 않아 뉴런의 출력값이 사라지는 현상을 방지할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecab8ae-ea24-40b5-9a19-f0bdb065ca2b",
   "metadata": {},
   "source": [
    "#### (5) 소프트맥스\n",
    "소프트맥스 함수는 다중 분류 모델에 사용하는 활성화 함수로 수식은 다음과 같습니다. \n",
    "\n",
    "$$\n",
    "\\text{Softmax}(\\mathbf{z}) = \\frac{e^{z_i}}{\\sum^k_{j=1} e^{z_i}}\\qquad \\text for \\,i = 1, 2,\\,…\\,k\n",
    "$$\n",
    "\n",
    "\n",
    "소프트맥스 함수는 모든 입력값을 0과 1 사이의 값으로 정규화하여 출력하며, 출력값들의 총합은 1이 됩니다. 분류할 클래스가 n개라고 할 때, 소프트맥스 출력값은 n개이며, 이는 입력이 각 클래스에 속할 확률 추정치입니다. \n",
    "예를 들어 분류할 클래스가 3개인 경우 소프트맥스 함수의 결과는 다음과 같습니다. \n",
    "\n",
    "\n",
    "수식은 조금 복잡하지만, 풀어서 설명하면 소프트맥스 함수는 분류하고자 하는 클래스가 3개일 때, n차원의 벡터를 입력받아서 합은 1이 되도록 모든 벡터 원소의 값을 0과 1 사이의 값으로 변경하여 3차원의 벡터를 반환하는 함수입니다. \n",
    "다음 예시와 같이 값들이 소프트맥스 함수를 통해 0.71, 0.05, 0.24와 같은 0과 1 사이의 값으로 변환되고, 변환된 값들의 총합은 1입니다. 소프트맥스 함수의 출력은 확률값이 되고, 이 중 가장 큰 확률값이 분류의 결과가 되며, 예시에서는 분류 결과가 클래스 A로 되었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0198898-8b44-43c0-8139-a836e2f0e196",
   "metadata": {},
   "source": [
    "## 2. 심층신경망\n",
    "### 1) 심층신경망 구조 알아보기\n",
    "입력층(input layer)과 출력층(outpyt layer) 사이에 여러 개의 은닉층(hidden layer)이 있는 인공신경망을 심층신경망(Deep Neural Network, DNN)이라고 합니다. \n",
    "\n",
    "심층신경망은 입력 데이터를 수신하는 입력층과 여러 개의 은닉층, 그리고 모델의 출력이 나오는 출력층으로 구성됩니다. <br>\n",
    "심층신경망에서 은닉층과 노드(뉴런)의 개수가 많아질수록, 입력 데이터 자체만으로는 미처 알지못한 높은 수준의 특징과 패턴을 찾아낼 수 있습니다. 심층신경망의 은닉층에서는 입력 데이터에서 높은 수준의 특성/패턴 데이터를 추출하고, 심층신경망의 출력층에서 분류/예측 모델이 해결하고자 하는 문제에 대한 알고리즘이 적용됩니다. <br>\n",
    "딥러닝 알고리즘을 이해하기 위해서는 다음 용어들을 알아야 합니다. \n",
    "<div style=\"display:table; border-collapse:collapse; width:100%; text-align:center;\">\n",
    "    <div style=\"display:table-row; background-color:#d9e2f3; font-weight:bold\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">용어</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">설명</div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">샘플</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">예측에 사용하는 데이터로, 입력(input)이라고도 표기합니다.</div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">입력</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">모델이 규칙을 찾아야 할 대상으로, 샘플(sample)과 같은 용어 입니다. </div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">타깃</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">모델이 예측해야 하는 대상으로, 샘플(입력)에 상응하는 레이블(label)입니다. </div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">학습</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">컴퓨터가 스스로 데이터의 규칙을 찾아내는 기술입니다. </div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">훈련</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">데이터의 규칙을 찾고 정확도를 높이는 방향으로 수정하는 과정, 즉 학습을 구현하기 위한 과정입니다. </div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">알고리즘</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">특정 문제를 해결하거나 계산을 수행하는 데 필요한 단계의 순서를 구체적으로 명시하는 규칙의 조합으로, 딥러닝에서 알고리즘은 입력 데이터를 학습해 ‘예측’이라는 결과를 출력하는 것을 의미합니다.</div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">모델</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">데이터 학습이 완료된 알고리즘으로, 수학식이나 샘플 데이터를 입력받아서 타깃 레이블 값을 예측하는 함수입니다. 딥러닝을 통해 최종적으로 얻고자 하는 산출물입니다. </div>\n",
    "    </div>    \n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">파라미터</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">모델을 규정하는 값으로 가중치와 편향이 있습니다. </div>        \n",
    "    </div>\n",
    "        <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">예측</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">모델치를 추정(estimate)하는 값입니다. </div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">손실</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">모델의 출력값과 타깃인 실제값 간의 차이를 비교하는 함수입니다. </div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">손실함수</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">모델의 예측인 출력값과 타깃인 실제값 간의 차이를 비교하는 함수입니다. </div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">경사하강법</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">어떤 손실함수가 정의되었을 때, 손실함수의 값이 최소가 되는 지점을 찾아가는 방법입니다. </div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">오차역전파</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">신경망에서 순전파 이후에 출력값에 오차가 있는 경우, 출력층에서 입력방향으로(역방향) 오차가 작아지도록 가중치가 더 이상 없데이트 되지 않을 때까지 반복하는 알고리즘입니다. </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb872ded-afd1-4b7d-b02f-163709bd1132",
   "metadata": {},
   "source": [
    "### 2) 신경망 출력 계산하기\n",
    "심층신경망의 출력 계산은 입력층에서 시작해서 정방향으로 훈련 데이터의 패턴을 신경망의 네트워크에 전파하는 순전파(forward propagation) 과정으로 이루어집니다. <br>\n",
    "신경망 출력 계산 방법을 설명하기 위해 $w^1_{12}$과 같은 표기를 사용합니다. 표기의 위첨자는 신경망의 몇 번째 층인지를 나타내는 인덱스 번호입니다. 그리고 표기의 아래첨자 앞자리 숫자는 앞 층에서 몇번 째 뉴런인지를, 아래첨자 뒷자리 숫자는 다음 층에서 몇 번째 뉴런인지를 나타내는 인덱스 번호입니다. <br>\n",
    "데이터의 패턴이 정방향으로 신경망의 네트워크로 전파될 때, 현 단계 뉴런의 가중치와 전 단계 뉴런의 출력값 곱들의 합을 입력값으로 받습니다. 이 같은 다시 활성화 함수를 통해 다음 뉴런으로 전파됩니다. <br>\n",
    "최종적으로 출력층에서 나오는 값은 이 모델이 예측한 결괏값입니다. 심층신경망을 출력을 기반으로 손실함수를 이용하여 오차를 계산하고, 심층신경망 네트워크에 있는 모든 가중치에 대한 도함수를 찾아서 오차를 역전파하여 모든 가중치와 편향을 업데이트합니다. <br>\n",
    "반복적인 순전파와 역전파를 통해 최적의 편향값들을 찾았다면 모델 학습이 잘된 것이며, 해당 출력을 모델의 예측값으로 사용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ca97ed-a0ac-4534-aaae-68787ccc30a9",
   "metadata": {},
   "source": [
    "### 3) 손실함수 이해하기\n",
    "손실함수는 신경망 학습의 목적 함수로 신경망의 출력값(예측값)과 실제값의 차이를 계산하는 함수입니다. 회귀 모델과 분류 모델에 주로 사용하는 손실함수를 알아봅니다. \n",
    "\n",
    "#### (1) 회귀 모델 손실함수\n",
    "수치를 예측하는 회귀 모델(Regression)은 손실함수로, 평균제곱오차(Mean Squared Error, MSE)를 주로 사용합니다. 평균제곱오차(MSE)는 수학적인 분석이 쉽고 계산이 용이합니다.<br> \n",
    "평균제곱오차는 실제값과 예측값의 차이인 오차들의 제곱 평균이어서 특이치에 민감합니다. 예측값과 실제값의 절댓값 차이인 평균절대오차(MAE)도 회귀 모델의 손실함수로 사용하며, MAE는 MSE보다 이상치에 덜 민감합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c6fd3-e086-48ff-9554-b077ca941753",
   "metadata": {},
   "source": [
    "#### (2) 분류 모델 손실함수\n",
    "카테고리를 분류하는 분류 모델(Classification)의 손실함수는 크로스 엔트로피(Cross Entropy)함수를 사용합니다. 크로스 엔트로피 함수는 실제값(레이블)과 모델의 예측값이 많이 틀릴수록, 그에 비례하여 더 큰 손실값이 부여되도록 디자인된 함수입니다. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "분류 모델의 종류에는 이진 분류와 다중 분류가 있습니다. 이진 분류는 True 또는 False, 양성 또는 음성 등 2개의 클래스로 분류하는 모델이며, 이진 교차 엔트로피 오차(Binary Cross Entropy Error, BCEE) 함수를 사용합니다. 이진 교차 엔트로피 오차함수 공식은 다음과 같습니다. \n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "여러 개의 레입르 중 하나로 분류하는 다중 분류 모델 훈련에는 범주형 교차 엔트로피 오차 함수를 사용합니다. 다중 분류는 분류해야 할 클래스가 3개 이상인 경우로 범주형 교차 엔트로피 오차함수의 공식은 다음과 같습니다. \n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba170182-ccc8-4b32-a34c-127da50a966f",
   "metadata": {},
   "source": [
    "#### 4) 경사하강법 이해하기\n",
    "딥러닝에서 학습 데이터를 입력하여 신경망 구조를 거쳐 결괏값(예측값)을 얻습니다. 모델이 예측한 값과 실제값의 차이인 오차를 최소화하는 파라미터를 찾는 과정이 최적화이며, 신경망 최적화기법으로 경사하강법(Gradient Descent)이 대표적입니다. <br>\n",
    "경사하강법은 오차가 최소화되도록 파라미터를 업데이트하는 데 손실함수의 기울기(gradient)를 사용합니다. <br>\n",
    "다음 그래프와 같이 손실함수는 가중치(w)에 대한 함수로, 손실함수가 볼록함수 형태라면 미분으로 손실이 가장 작은 가중치를 찾을 수 있습니다. 하지만 딥러닝에서는 손실함수가 복잡하고 계산량이 매우 크며 미분이 0이 되는 값이 여러 개 존재하므로 미분만으로 최솟값을 찾기 어려워 경사하강법을 사용합니다. 아래 그림에서 보듯이, 경사하강법은 손실함수의 현재 가중치에서 기울기를 구해 손실을 줄이는 방향으로 가중치를 업데이트해 나갑니다. \n",
    "\n",
    "\n",
    "경사하강법의 단점을 보완한 방법으로 확률적 경사하강법, 미니배치 경사하강법이 있습니다. 경사하강법은 정확하게 가중치를 찾아가지만 파라미터를 한번 업데이트하는 데 데이터세트 전체를 사용합니다. 딥러닝 모델의 학습에서 사용하는 데이터세트 규모는 보통 수백만 개가 넘는데, 많은 계산량 때문에 속도가 느리거나 최적해를 찾기 전에 학습을 멈출 수 있는 단점이 있습니다. <br>\n",
    "확률적 경사하강법은 파라미터를 업데이트하기 위해 무작위로 샘플링된 1개의 데이터를 사용하는 방법이며, 적은 계산량으로 적절한 기울기를 구할 수 있지만 노이즈가 심하다는 단점이 있습니다. <br>\n",
    "미니배치 경사하강법은 랜덤 추출한 일부 데이터를 통해 가중치를 조절합니다. 예를 들어 학습 데이터가 1,000개이고 배치 크기를 100으로 하면, 총 10개의 미니배치가 나오고, 이 미니배치 하나 당 한 번씩 경사하강법을 진행하여 속도를 개선합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393c4cf4-d103-4b41-b416-3a061f2cf9ef",
   "metadata": {},
   "source": [
    "경사하강법의 최적화 기법에는 관성의 효과를 내는 모멘텀(Momentum), 파라미터의 업데이트 횟수에 따라 학습률을 조절하는 옵션이 추가된 아다그라드, 지수 이동평균을 이용하는 알엠에스프롭, 모멘텀과 알엠에스프롭을 융합한 방법인 아담 등 다양합니다. 특히 아담은 모멘텀과 알엠에스프롭을 섞어놓은 최적화 알고리즘으로 딥러닝에서 자주 사용됩니다. \n",
    "\n",
    "### 5) 역전파 알고리즘 이해하기\n",
    "신경망 학습 방법은 모델의 출력값과 실제값의 차이인 오차값을 최소화하기 위해 모델의 파라미터를 조정하는 오차역전파 알고리즘을 사용합니다. <br>\n",
    "심층신경망의 출력값은 각 노드들에서 계산되는 가중합들의 연산 결과이므로 가중치와 편향이 모델에서 변경 가능한 파라미터이고, 오차값이 적은 출력값을 만드는 최적의 가중치들과 편향값들을 찾는 것이 딥러닝 모델의 목적입니다. <br>\n",
    "처음에는 심층신경망 각 노드(뉴럽)의 가중치와 편향을 무작위로 부여하고, 순전파와 오차역전파를 계속 반복하는 방식으로 가중치들과 편향값들을 조정하면서 오차를 최소화합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c594fbc7-89ea-43f0-ab9c-60602ed3a7b5",
   "metadata": {},
   "source": [
    "다음 그림과 같이 어떤 입력에 대한 실제값이 10인 경우, 신경망의 정방향 계산에 의한 출력값이 7이라면 오차는 실제값 10에서 신경망의 출력값 7을 뺀 3이 됩니다. \n",
    "\n",
    "출력층에서 측정한 오차를 이전의 은닉층과 입력층으로 역방향 전파하면서 가중치들과 편향값을 조정합니다. 이때 역방향으로 전파되는 오차의 크기는 각 뉴런이 결괏값에 미치는 영향도에 비례합니다. 예를 들어 가중치 $w^2_{11}$에는 1.5, $w^2_{12}$에는 1.2, $w^2_{13}$에는 0.3만큼의 오차를 줄일 수 있도록 가중치 값을 조정합니다. \n",
    "\n",
    "**■ 역전파 알고리즘 동작 방식**<br>\n",
    "① 신경망 모델은 모든 가중치(w)와 편향(b)을 랜덤하게 초기화합니다.<br>\n",
    "② 신경망 모델에 x값을 입력하면 신경망은 예측 y를 출력합니다. <br>\n",
    "③ 신경망에서 손실 최소화를 위해 가중치를 업데이트해야 합니다. <br>\n",
    "④ 역전파를 사용해 신경망에 있는 모든 가중치에 대한 비용함수의 기울기를 계산합니다. <br>\n",
    "⑤ 가중치에 대한 비용함수의 기울기에 비례하여 가중치를 조정함으로써, 역전파는 비용을 감소하는 방향으로 가중치를 변경할 수 있습니다. \n",
    "\n",
    "#### 6) 드롭아웃 이해하기\n",
    "보통 사용하는 심층신경망은 파라미터가 매우 많은 모델입니다. 파라미터가 많으면 모델의 복잡도가 높은 것이고, 모델의 복잡도가 너무 높으면 과적합(overfitting) 경향이 발생합니다. 심층신경망이 훈련 데이터를 외워버리면 훈련 데이터세트에서는 좋은 성능을 내지만, 별도의 테스트 데이터세트나 새로운 데이터세트에서는 성능이 낮습니다.<br> \n",
    "드롭아웃(Dropout)은 심층신경망에 적용하여 과대적합을 방지하고 일반화 성능을 높이는 방법입니다. 드롭아웃은 심층신경망을 훈련하는 동안, 반복마다 은닉 유닛의 일부를 확률 p만큼 랜덤하게 드롭해서 학습에 참여하지 않도록 하는 방법입니다. 드롭아웃 확률은 사용자가 지정하고, p=0.5를 많이 사용하여 이 경우 50%의 은닉 노드가 랜덤하게 드롭됩니다. 데이터를 신경망에 순전파하는 과정에서 은닉 노드가 생략되므로 역전파에서도 제외됩니다. 일반화는 모델 훈련에 사용된 데이터가 아닌 이전에 접하지 못한 새로운 데이터에 대한 올바른 에측을 수행하는 능력을 의미합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e9228-9bf5-4460-b82a-43c8b6094914",
   "metadata": {},
   "source": [
    "## 3. 딥러닝 프레임워크\n",
    "### 1) 텐서 이해하기\n",
    "텐서(Tensor)는 다차원 배열을 통칭하는 용어입니다. 스칼라는 값을 1개의 수치로 표현하고, 벡터는 하나의 값을 2개 이상의 수치로 표협합니다. 그리고 매트릭스는 2개 이상의 벡터 값을 통합해 구성된 값입니다. <br>\n",
    "행렬을 2차원의 배열이라고 한다면 텐서는 2차원 이상의 다차원을 가진 배열로 이해하면 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55f2bb0-cd33-4a8a-9e8b-3307b716db76",
   "metadata": {},
   "source": [
    "#### 2) 딥러닝 개발 프레임워크 알아보기\n",
    "딥러닝 모델을 만들고 사용하기 위한 대표적인 프레임워크에는 텐서플로, 케라스, 파이토치가 있으며, 오픈소스로 제공됩니다. <br>\n",
    "딥러닝 프레임워크는 딥러닝에 필요한 많은 알고리즘을 제공하기 때문에, 이를 활용할 경우 복잡한 로직이나 수학 연산을 직접 구현할 필요 없이 솔루션 개발에 집중할 수 있습니다. \n",
    "\n",
    "**■ 텐서플로(TensorFlow)**<br>\n",
    "- 구글이 개발하여 오픈소스로 공개한 딥러닝 프레임워크로 데이터 플로 그래프(Data Flow Graph) 구조를 사용하여 수치 연산을 합니다. <br>\n",
    "- 강력한 시각화 기능과 높은 수준의 모델 개발에 사용할 수 있는 여러 옵션을 갖춘 딥러닝 라이브러리입니다. <br>\n",
    "- 프로덕션 레디 배포(production-ready deployment) 옵션과 모바일 플랫폼 지원에 장점이 있습니다. <br>\n",
    "- 텐서플로 홈페이지：https://www.tensorlfow.org\n",
    "\n",
    "**■ 케라스(Keras)**<br>\n",
    "- 텐서플로의 상위 레발 API로 딥러닝 모델을 쉽게 구현하는데 도움을 주는 직관적인 API를 제공합니다. <br>\n",
    "- 시퀀셜(Sequential) 모델로 원하는 레이어(layer)를 쉽게 순차적으로 쌓을 수 있습니다. <br>\n",
    "- 다중 출력 등 복잡한 모델을 케라스 함수 API로 쉽게 구성할 수 있습니다. <br>\n",
    "- 케라스 홈페이지：https://keras.io\n",
    "\n",
    "**■ 파이토치(PyTorch)**<br>\n",
    "- 페이스북이 개발한 파이썬 기반 오픈소스 라이브러리 토치(Torch)를 기반으로 만들어진 프레임워크입니다.<br>\n",
    "- 동적 계산 그래프를 사용하며 파이썬 친화적인 프레임워크입니다.<br>\n",
    "- 파이토치 홈페이지：https://pytorch.org\n",
    "  \n",
    "#### 3) 텐서플로 설치 및 활용하기\n",
    "딥러닝 모델 개발 실습에 텐서플로를 사용해 봅니다. 파이썬이 설치된 환경에서 파이썬 라이브러리 관리 툴 pip로 텐서플로를 설치합니다. <br>\n",
    "- 터미널/명령창：pip install tensorflow<br>\n",
    "- 주피터 노트북：!pip install tensorflow\n",
    "\n",
    "주피터 노트북에서 텐서플로를 설치합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "061e7f7e-9bf5-4b65-98ff-e80dc871e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6d5c475-56ff-45b0-8804-f840c90fbc2a",
   "metadata": {},
   "source": [
    "#254"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc45bd3-b27e-43e6-aea0-6ea34b9909e3",
   "metadata": {},
   "source": [
    "설치가 완료되면 다음 코드로 텐서플로 버전을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff389aca-2afc-46dc-a859-ac0a63d4f4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "# 설치된 텐서플로 버전 확인하기\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845571bf-b959-4c70-b209-f02cf88d73c7",
   "metadata": {},
   "source": [
    "먼저 필요한 라이브러리를 불러오고(import) 딥러닝 모델 학습에 사용할 데이터를 생성합니다. 딥러닝 모델에 훈련 데이터로 사용하는 데이터 x_train은 넘파이 reshape() 메소드로 데이터의 형태를 바꿔줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10fabaea-3035-4425-9399-cf47431c6a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터 : [[ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]]\n",
      "입력 데이터 형태: (10, 1)\n",
      "출력 데이터: [ 3  5  7  9 11 13 15 17 19 21]\n",
      "출력 데이터 형태 : (10,)\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# 모델 학습 데이터 생성하기\n",
    "x = [1, 2, 3, 4,  5, 6,  7,  8,  9, 10]\n",
    "y = [3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "x_train = np.array(x)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "y_train = np.array(y)\n",
    "\n",
    "print(f'입력 데이터 : {x_train}')\n",
    "print(f'입력 데이터 형태: {x_train.shape}')\n",
    "print(f'출력 데이터: {y_train}')\n",
    "print(f'출력 데이터 형태 : {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be411cbb-ea8b-48fb-8f84-46f0c8be94cf",
   "metadata": {},
   "source": [
    "케라스의 Sequential 클래스로 층(layer)을 선형으로 연결하는 신경망을 구성합니다.<p>\n",
    "- Sequential 객체를 생성하고 add() 메소드로 층을 추가합니다.<br>\n",
    "- Sequential 모델의 첫 번째 층은 input_shape 또는 input_dim 인자로 입력 형태에 대한 정보를 받습니다.  두 번째 이후 충돌은 앞선 층의 출력과 연결되기 때문에 자동으로 형태를 추정할 수 있으므로 형태 정보를 갖고 있을 필요는 없습니다. <br>\n",
    "- 케라스 layers API에서는 Dense layer, Activation layer, Conv2D layer 등 다양한 층을 제공합니다. 여기에서는 기본적인 Dense 층을 사용하며, Dense 층은 한 층의 유닛이 다른 층의 모든 유닛과 연결된 상태를 의미합니다. Dense 클래스의 주요 매개변수들은 다음과 같습니다. <br>\n",
    "  - units：출력 유닛 개수를 설정합니다. <br>\n",
    "    - input_shape：입력의 개수를 설정합니다. <br>\n",
    "  - activation：활성화 함수를 설정합니다. <br>\n",
    "  - liner：기본값으로 입력값과 가중치로 계산된 결괏값이 그대로 출력됩니다. <br>\n",
    "  - sigmoid：스기모이드 함수로 이진 분류 출력층에 사용합니다. <br>\n",
    "  - softmax：소프트맥스 함수로 다중클래스 분류 문제 출력층에 사용합니다. <br>\n",
    "  - relu：렐루(Rectified Linear Unit) 함수로 은닉층에서 주로 사용합니다. <p>\n",
    "\n",
    "Conv 2D 등 다른 레이어에 대한 정보는 케라스 API 페이지(https://keras.io/api/layers)를 참고합니다. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "121f7cba-9a64-4099-93db-66ea21bdb546",
   "metadata": {},
   "source": [
    "#256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64474cfa-af65-4d1a-a6eb-99079c0763ea",
   "metadata": {},
   "source": [
    "그리고 summary() 메소드로 신경망의 구성을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6deb739a-3862-4508-b2a4-3dd1d8b66495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2 (8.00 Byte)\n",
      "Trainable params: 2 (8.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Keras의 Sequential 모델 구성하기\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=42) #모델 시드 고정하기\n",
    "model = Sequential()\n",
    "model.add(Dense(units=1, input_shape=(1,),kernel_initializer=initializer))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615cc4ae-0ed7-46a1-901f-c86abca4bce3",
   "metadata": {},
   "source": [
    "compile 메소드로 모델의 학습에 대한 환경을 설정합니다. <br>\n",
    "compile 메소드의 매개변수는 다음과 같습니다. <br>\n",
    "- optimizer：optimizer 알고리즘으로 sgd, adam 등이 있습니다.<br> \n",
    "- loss：손실함수로 mse, categorical_crossentropy 등이 있습니다. <br>\n",
    "- metric：모델의 성능을 평가하는 데 사용되는 함수로 mae, acc 등이 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b449e9c6-9a87-4e42-91fd-4d44ed1ecdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법 설정하기\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096da6e-0e56-48f5-bf04-ac16fc4450d5",
   "metadata": {},
   "source": [
    "모델을 학습시키기 위해서는 일반적으로 fit 함수를 사용합니다. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e3dd419-4ab6-4111-b0c5-616598d82d5a",
   "metadata": {},
   "source": [
    "#257"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0615d0e6-ffe3-4d03-89d5-c198ea3d5b90",
   "metadata": {},
   "source": [
    "fit 메소드의 인자는 다음과 같습니다. <br>\n",
    "- x：입력데이터<br>\n",
    "- y：타깃 데이터(label)<br>\n",
    "- batch_size：한 번에 학습할 때 사용하는 데이터 개수로 batch_size가 10이라면 10개의 데이터를 학습한 다음 가중치를 한번 갱신합니다. <br>\n",
    "- epochs：학습 데이터 반복 횟수로 학습 데이터세트 전체를 몇 번 학습하는지를 의미합니다. 학습 데이터세트를 여러 번 학습할수록 학습 효과는 커집니다. 하지만 학습을 많이 하면 모델의 가중치가 학습 데이터에 지나치게 최적화되는 과대적합 현상이 발생합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "224954e9-64bc-4ee8-8799-0f78b2869df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 323.2251 - mae: 16.1452\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 14.9011 - mae: 3.5555\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7542 - mae: 0.8584\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1045 - mae: 0.2802\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0741 - mae: 0.2255\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0721 - mae: 0.2238\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0715 - mae: 0.2230\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0709 - mae: 0.2223\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0703 - mae: 0.2214\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0697 - mae: 0.2205\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0691 - mae: 0.2196\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0685 - mae: 0.2187\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0679 - mae: 0.2178\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0674 - mae: 0.2168\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0668 - mae: 0.2159\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0663 - mae: 0.2150\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0657 - mae: 0.2141\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0651 - mae: 0.2132\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0646 - mae: 0.2123\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0641 - mae: 0.2114\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0635 - mae: 0.2106\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0630 - mae: 0.2097\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0625 - mae: 0.2088\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0619 - mae: 0.2079\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0614 - mae: 0.2070\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0609 - mae: 0.2062\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0604 - mae: 0.2053\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0599 - mae: 0.2044\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0594 - mae: 0.2036\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0589 - mae: 0.2027\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0584 - mae: 0.2019\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0579 - mae: 0.2010\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0574 - mae: 0.2002\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0569 - mae: 0.1993\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0565 - mae: 0.1985\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0560 - mae: 0.1977\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0555 - mae: 0.1968\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0551 - mae: 0.1960\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0546 - mae: 0.1952\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0541 - mae: 0.1944\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0537 - mae: 0.1936\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0532 - mae: 0.1927\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0528 - mae: 0.1919\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0523 - mae: 0.1911\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0519 - mae: 0.1903\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0515 - mae: 0.1895\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0510 - mae: 0.1887\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0506 - mae: 0.1879\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0502 - mae: 0.1871\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0498 - mae: 0.1864\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0493 - mae: 0.1856\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0489 - mae: 0.1848\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0485 - mae: 0.1840\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0481 - mae: 0.1832\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0477 - mae: 0.1825\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0473 - mae: 0.1817\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0469 - mae: 0.1810\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0465 - mae: 0.1802\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0461 - mae: 0.1794\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0457 - mae: 0.1787\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0454 - mae: 0.1779\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0450 - mae: 0.1772\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0446 - mae: 0.1764\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0442 - mae: 0.1757\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0439 - mae: 0.1750\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0435 - mae: 0.1742\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0431 - mae: 0.1735\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0428 - mae: 0.1728\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0424 - mae: 0.1720\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0421 - mae: 0.1713\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0417 - mae: 0.1706\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0414 - mae: 0.1699\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0410 - mae: 0.1692\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0407 - mae: 0.1685\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0403 - mae: 0.1677\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0400 - mae: 0.1670\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0396 - mae: 0.1663\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0393 - mae: 0.1656\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0390 - mae: 0.1649\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0387 - mae: 0.1643\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0383 - mae: 0.1636\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0380 - mae: 0.1629\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0377 - mae: 0.1622\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0374 - mae: 0.1615\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0371 - mae: 0.1608\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0368 - mae: 0.1602\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0364 - mae: 0.1595\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0361 - mae: 0.1588\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0358 - mae: 0.1582\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0355 - mae: 0.1575\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0352 - mae: 0.1568\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0349 - mae: 0.1562\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0347 - mae: 0.1555\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0344 - mae: 0.1549\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0341 - mae: 0.1542\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0338 - mae: 0.1536\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0335 - mae: 0.1529\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0332 - mae: 0.1523\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0329 - mae: 0.1516\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0327 - mae: 0.1510\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0324 - mae: 0.1504\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0321 - mae: 0.1497\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0319 - mae: 0.1491\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0316 - mae: 0.1485\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0313 - mae: 0.1479\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0311 - mae: 0.1472\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0308 - mae: 0.1466\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0305 - mae: 0.1460\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0303 - mae: 0.1454\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0300 - mae: 0.1448\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0298 - mae: 0.1442\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0295 - mae: 0.1436\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0293 - mae: 0.1430\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0290 - mae: 0.1424\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0288 - mae: 0.1418\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0286 - mae: 0.1412\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0283 - mae: 0.1406\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0281 - mae: 0.1400\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0278 - mae: 0.1394\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0276 - mae: 0.1388\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0274 - mae: 0.1382\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0271 - mae: 0.1376\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0269 - mae: 0.1371\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0267 - mae: 0.1365\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0265 - mae: 0.1359\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0262 - mae: 0.1353\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0260 - mae: 0.1348\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0258 - mae: 0.1342\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0256 - mae: 0.1336\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0254 - mae: 0.1331\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0252 - mae: 0.1325\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0250 - mae: 0.1320\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0247 - mae: 0.1314\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0245 - mae: 0.1309\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0243 - mae: 0.1303\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0241 - mae: 0.1298\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0239 - mae: 0.1292\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0237 - mae: 0.1287\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0235 - mae: 0.1281\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0233 - mae: 0.1276\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0231 - mae: 0.1271\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0229 - mae: 0.1265\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0227 - mae: 0.1260\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0226 - mae: 0.1255\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0224 - mae: 0.1249\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0222 - mae: 0.1244\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0220 - mae: 0.1239\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0218 - mae: 0.1234\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0216 - mae: 0.1229\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0214 - mae: 0.1223\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0213 - mae: 0.1218\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0211 - mae: 0.1213\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0209 - mae: 0.1208\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0207 - mae: 0.1203\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0206 - mae: 0.1198\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0204 - mae: 0.1193\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0202 - mae: 0.1188\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0201 - mae: 0.1183\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0199 - mae: 0.1178\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0197 - mae: 0.1173\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0196 - mae: 0.1168\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0194 - mae: 0.1163\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0192 - mae: 0.1158\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0191 - mae: 0.1153\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0189 - mae: 0.1149\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0187 - mae: 0.1144\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0186 - mae: 0.1139\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0184 - mae: 0.1134\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0183 - mae: 0.1129\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0181 - mae: 0.1125\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0180 - mae: 0.1120\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0178 - mae: 0.1115\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0177 - mae: 0.1111\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0175 - mae: 0.1106\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0174 - mae: 0.1101\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0172 - mae: 0.1097\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0171 - mae: 0.1092\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0169 - mae: 0.1087\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0168 - mae: 0.1083\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0167 - mae: 0.1078\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0165 - mae: 0.1074\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0164 - mae: 0.1069\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0162 - mae: 0.1065\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0161 - mae: 0.1060\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0160 - mae: 0.1056\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0158 - mae: 0.1051\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0157 - mae: 0.1047\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0156 - mae: 0.1043\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0154 - mae: 0.1038\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0153 - mae: 0.1034\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0152 - mae: 0.1030\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0151 - mae: 0.1025\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0149 - mae: 0.1021\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0148 - mae: 0.1017\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0147 - mae: 0.1012\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0146 - mae: 0.1008\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0144 - mae: 0.1004\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0143 - mae: 0.1000\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0142 - mae: 0.0995\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0141 - mae: 0.0991\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0140 - mae: 0.0987\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0138 - mae: 0.0983\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0137 - mae: 0.0979\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0136 - mae: 0.0975\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0135 - mae: 0.0971\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0134 - mae: 0.0967\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0133 - mae: 0.0963\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0132 - mae: 0.0958\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0131 - mae: 0.0954\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0129 - mae: 0.0950\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0128 - mae: 0.0946\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0127 - mae: 0.0942\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0126 - mae: 0.0939\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0125 - mae: 0.0935\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0124 - mae: 0.0931\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0123 - mae: 0.0927\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0122 - mae: 0.0923\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0121 - mae: 0.0919\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0120 - mae: 0.0915\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0119 - mae: 0.0911\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0118 - mae: 0.0907\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0117 - mae: 0.0904\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0116 - mae: 0.0900\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0115 - mae: 0.0896\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0114 - mae: 0.0892\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0113 - mae: 0.0889\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0112 - mae: 0.0885\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0111 - mae: 0.0881\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0110 - mae: 0.0877\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0109 - mae: 0.0874\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0108 - mae: 0.0870\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0108 - mae: 0.0866\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0107 - mae: 0.0863\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0859\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0105 - mae: 0.0856\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0104 - mae: 0.0852\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0103 - mae: 0.0848\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0102 - mae: 0.0845\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0101 - mae: 0.0841\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0101 - mae: 0.0838\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0100 - mae: 0.0834\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0099 - mae: 0.0831\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0098 - mae: 0.0827\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0097 - mae: 0.0824\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0096 - mae: 0.0820\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0096 - mae: 0.0817\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0095 - mae: 0.0813\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0094 - mae: 0.0810\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0093 - mae: 0.0807\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0092 - mae: 0.0803\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0092 - mae: 0.0800\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0091 - mae: 0.0796\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0090 - mae: 0.0793\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0089 - mae: 0.0790\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0089 - mae: 0.0786\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0088 - mae: 0.0783\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0087 - mae: 0.0780\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0086 - mae: 0.0777\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0086 - mae: 0.0773\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0085 - mae: 0.0770\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0084 - mae: 0.0767\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0084 - mae: 0.0764\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0083 - mae: 0.0760\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0082 - mae: 0.0757\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0081 - mae: 0.0754\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0081 - mae: 0.0751\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0080 - mae: 0.0748\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0079 - mae: 0.0745\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0079 - mae: 0.0741\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0078 - mae: 0.0738\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0077 - mae: 0.0735\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0077 - mae: 0.0732\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0076 - mae: 0.0729\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0076 - mae: 0.0726\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0075 - mae: 0.0723\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0074 - mae: 0.0720\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0074 - mae: 0.0717\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0073 - mae: 0.0714\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0072 - mae: 0.0711\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0072 - mae: 0.0708\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0071 - mae: 0.0705\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0071 - mae: 0.0702\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0070 - mae: 0.0699\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0069 - mae: 0.0696\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0069 - mae: 0.0693\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0068 - mae: 0.0690\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0068 - mae: 0.0687\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0067 - mae: 0.0684\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0067 - mae: 0.0682\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0066 - mae: 0.0679\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0065 - mae: 0.0676\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0065 - mae: 0.0673\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0064 - mae: 0.0670\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0064 - mae: 0.0667\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0063 - mae: 0.0665\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0063 - mae: 0.0662\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0062 - mae: 0.0659\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0062 - mae: 0.0656\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0061 - mae: 0.0654\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0061 - mae: 0.0651\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0060 - mae: 0.0648\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0060 - mae: 0.0645\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0059 - mae: 0.0643\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0059 - mae: 0.0640\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0058 - mae: 0.0637\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0058 - mae: 0.0635\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0057 - mae: 0.0632\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0057 - mae: 0.0629\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0056 - mae: 0.0627\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0056 - mae: 0.0624\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0055 - mae: 0.0621\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0055 - mae: 0.0619\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0054 - mae: 0.0616\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0054 - mae: 0.0614\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0053 - mae: 0.0611\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0053 - mae: 0.0608\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0053 - mae: 0.0606\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0052 - mae: 0.0603\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0052 - mae: 0.0601\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0051 - mae: 0.0598\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0051 - mae: 0.0596\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0050 - mae: 0.0593\n",
      "Epoch 323/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0050 - mae: 0.0591\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0050 - mae: 0.0588\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0049 - mae: 0.0586\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0049 - mae: 0.0583\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0048 - mae: 0.0581\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0048 - mae: 0.0578\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0048 - mae: 0.0576\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0047 - mae: 0.0574\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0047 - mae: 0.0571\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0046 - mae: 0.0569\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0046 - mae: 0.0566\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0046 - mae: 0.0564\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0045 - mae: 0.0562\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0045 - mae: 0.0559\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0044 - mae: 0.0557\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0044 - mae: 0.0555\n",
      "Epoch 339/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0044 - mae: 0.0552\n",
      "Epoch 340/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0043 - mae: 0.0550\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0043 - mae: 0.0548\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0043 - mae: 0.0545\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0042 - mae: 0.0543\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0042 - mae: 0.0541\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0042 - mae: 0.0539\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0041 - mae: 0.0536\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0041 - mae: 0.0534\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0041 - mae: 0.0532\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0040 - mae: 0.0530\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0040 - mae: 0.0527\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0040 - mae: 0.0525\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0039 - mae: 0.0523\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0039 - mae: 0.0521\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0039 - mae: 0.0518\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0038 - mae: 0.0516\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0038 - mae: 0.0514\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0038 - mae: 0.0512\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0037 - mae: 0.0510\n",
      "Epoch 359/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0037 - mae: 0.0508\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0037 - mae: 0.0506\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0036 - mae: 0.0503\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0036 - mae: 0.0501\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0036 - mae: 0.0499\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0035 - mae: 0.0497\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0035 - mae: 0.0495\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0035 - mae: 0.0493\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0035 - mae: 0.0491\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0034 - mae: 0.0489\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0034 - mae: 0.0487\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0034 - mae: 0.0485\n",
      "Epoch 371/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0033 - mae: 0.0483\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0033 - mae: 0.0481\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0033 - mae: 0.0479\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0033 - mae: 0.0477\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0032 - mae: 0.0475\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0032 - mae: 0.0473\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0032 - mae: 0.0471\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0031 - mae: 0.0469\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0031 - mae: 0.0467\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0031 - mae: 0.0465\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0031 - mae: 0.0463\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0030 - mae: 0.0461\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0030 - mae: 0.0459\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0030 - mae: 0.0457\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0030 - mae: 0.0455\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0029 - mae: 0.0453\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0029 - mae: 0.0451\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0029 - mae: 0.0449\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0029 - mae: 0.0447\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0028 - mae: 0.0446\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0444\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0442\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0028 - mae: 0.0440\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0438\n",
      "Epoch 395/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0027 - mae: 0.0436\n",
      "Epoch 396/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0027 - mae: 0.0434\n",
      "Epoch 397/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0027 - mae: 0.0433\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0431\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0026 - mae: 0.0429\n",
      "Epoch 400/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0026 - mae: 0.0427\n",
      "Epoch 401/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0425\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0026 - mae: 0.0424\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0026 - mae: 0.0422\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0025 - mae: 0.0420\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0418\n",
      "Epoch 406/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0025 - mae: 0.0417\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0025 - mae: 0.0415\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0024 - mae: 0.0413\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0024 - mae: 0.0411\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0024 - mae: 0.0410\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0024 - mae: 0.0408\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0024 - mae: 0.0406\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0023 - mae: 0.0404\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0023 - mae: 0.0403\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0023 - mae: 0.0401\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0023 - mae: 0.0399\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0023 - mae: 0.0398\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0022 - mae: 0.0396\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0022 - mae: 0.0394\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0022 - mae: 0.0393\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0022 - mae: 0.0391\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0022 - mae: 0.0389\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0022 - mae: 0.0388\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0021 - mae: 0.0386\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0021 - mae: 0.0385\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0021 - mae: 0.0383\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0021 - mae: 0.0381\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0021 - mae: 0.0380\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0020 - mae: 0.0378\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0020 - mae: 0.0377\n",
      "Epoch 431/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0020 - mae: 0.0375\n",
      "Epoch 432/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0020 - mae: 0.0373\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0020 - mae: 0.0372\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0020 - mae: 0.0370\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0019 - mae: 0.0369\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0019 - mae: 0.0367\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0019 - mae: 0.0366\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0019 - mae: 0.0364\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0019 - mae: 0.0363\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0019 - mae: 0.0361\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0019 - mae: 0.0360\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0018 - mae: 0.0358\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0018 - mae: 0.0357\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0018 - mae: 0.0355\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0018 - mae: 0.0354\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0018 - mae: 0.0352\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0018 - mae: 0.0351\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0017 - mae: 0.0349\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0017 - mae: 0.0348\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0017 - mae: 0.0346\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0017 - mae: 0.0345\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0017 - mae: 0.0343\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0017 - mae: 0.0342\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0017 - mae: 0.0340\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0016 - mae: 0.0339\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0016 - mae: 0.0338\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0016 - mae: 0.0336\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0016 - mae: 0.0335\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0016 - mae: 0.0333\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0016 - mae: 0.0332\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0016 - mae: 0.0331\n",
      "Epoch 462/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0016 - mae: 0.0329\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0015 - mae: 0.0328\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0015 - mae: 0.0326\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0015 - mae: 0.0325\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0015 - mae: 0.0324\n",
      "Epoch 467/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0015 - mae: 0.0322\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0015 - mae: 0.0321\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0015 - mae: 0.0320\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0015 - mae: 0.0318\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0014 - mae: 0.0317\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0014 - mae: 0.0316\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0014 - mae: 0.0314\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0014 - mae: 0.0313\n",
      "Epoch 475/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0014 - mae: 0.0312\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0014 - mae: 0.0310\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0014 - mae: 0.0309\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0014 - mae: 0.0308\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0013 - mae: 0.0306\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0013 - mae: 0.0305\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0013 - mae: 0.0304\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0013 - mae: 0.0303\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0013 - mae: 0.0301\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0013 - mae: 0.0300\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0013 - mae: 0.0299\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0013 - mae: 0.0297\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0013 - mae: 0.0296\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0012 - mae: 0.0295\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0012 - mae: 0.0294\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0012 - mae: 0.0293\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0012 - mae: 0.0291\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0012 - mae: 0.0290\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0012 - mae: 0.0289\n",
      "Epoch 494/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0012 - mae: 0.0288\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0012 - mae: 0.0286\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0012 - mae: 0.0285\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0012 - mae: 0.0284\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0011 - mae: 0.0283\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0011 - mae: 0.0282\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0011 - mae: 0.0280\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0011 - mae: 0.0279\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0011 - mae: 0.0278\n",
      "Epoch 503/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0011 - mae: 0.0277\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0011 - mae: 0.0276\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0011 - mae: 0.0275\n",
      "Epoch 506/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0011 - mae: 0.0273\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0011 - mae: 0.0272\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0011 - mae: 0.0271\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0010 - mae: 0.0270\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0010 - mae: 0.0269\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0010 - mae: 0.0268\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0010 - mae: 0.0267\n",
      "Epoch 513/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0010 - mae: 0.0266\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0010 - mae: 0.0264\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.9354e-04 - mae: 0.0263\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.8521e-04 - mae: 0.0262\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.7696e-04 - mae: 0.0261\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.6877e-04 - mae: 0.0260\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.6064e-04 - mae: 0.0259\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.5259e-04 - mae: 0.0258\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.4462e-04 - mae: 0.0257\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.3669e-04 - mae: 0.0256\n",
      "Epoch 523/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2885e-04 - mae: 0.0255\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.2105e-04 - mae: 0.0254\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.1334e-04 - mae: 0.0252\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.0569e-04 - mae: 0.0251\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.9809e-04 - mae: 0.0250\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.9057e-04 - mae: 0.0249\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.8311e-04 - mae: 0.0248\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.7570e-04 - mae: 0.0247\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.6837e-04 - mae: 0.0246\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.6108e-04 - mae: 0.0245\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.5385e-04 - mae: 0.0244\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.4669e-04 - mae: 0.0243\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.3962e-04 - mae: 0.0242\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.3259e-04 - mae: 0.0241\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.2559e-04 - mae: 0.0240\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.1868e-04 - mae: 0.0239\n",
      "Epoch 539/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1183e-04 - mae: 0.0238\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0501e-04 - mae: 0.0237\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.9826e-04 - mae: 0.0236\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.9156e-04 - mae: 0.0235\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.8493e-04 - mae: 0.0234\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.7836e-04 - mae: 0.0233\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.7183e-04 - mae: 0.0232\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.6535e-04 - mae: 0.0231\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.5895e-04 - mae: 0.0230\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.5259e-04 - mae: 0.0229\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.4627e-04 - mae: 0.0228\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.4002e-04 - mae: 0.0227\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.3383e-04 - mae: 0.0226\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.2768e-04 - mae: 0.0225\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.2157e-04 - mae: 0.0224\n",
      "Epoch 554/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.1554e-04 - mae: 0.0223\n",
      "Epoch 555/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0952e-04 - mae: 0.0223\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0359e-04 - mae: 0.0222\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9770e-04 - mae: 0.0221\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9183e-04 - mae: 0.0220\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.8603e-04 - mae: 0.0219\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.8029e-04 - mae: 0.0218\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.7458e-04 - mae: 0.0217\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.6894e-04 - mae: 0.0216\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.6334e-04 - mae: 0.0215\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.5776e-04 - mae: 0.0214\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.5225e-04 - mae: 0.0213\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.4679e-04 - mae: 0.0212\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.4138e-04 - mae: 0.0212\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.3598e-04 - mae: 0.0211\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.3065e-04 - mae: 0.0210\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.2536e-04 - mae: 0.0209\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.2012e-04 - mae: 0.0208\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.1494e-04 - mae: 0.0207\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.0978e-04 - mae: 0.0206\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.0467e-04 - mae: 0.0205\n",
      "Epoch 575/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.9961e-04 - mae: 0.0205\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.9459e-04 - mae: 0.0204\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.8958e-04 - mae: 0.0203\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.8465e-04 - mae: 0.0202\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.7975e-04 - mae: 0.0201\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.7490e-04 - mae: 0.0200\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.7008e-04 - mae: 0.0199\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.6530e-04 - mae: 0.0199\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.6056e-04 - mae: 0.0198\n",
      "Epoch 584/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.5586e-04 - mae: 0.0197\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.5119e-04 - mae: 0.0196\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.4658e-04 - mae: 0.0195\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.4199e-04 - mae: 0.0194\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.3746e-04 - mae: 0.0194\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.3295e-04 - mae: 0.0193\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.2850e-04 - mae: 0.0192\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.2405e-04 - mae: 0.0191\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.1966e-04 - mae: 0.0190\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.1531e-04 - mae: 0.0190\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.1098e-04 - mae: 0.0189\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.0671e-04 - mae: 0.0188\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.0246e-04 - mae: 0.0187\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.9823e-04 - mae: 0.0186\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.9408e-04 - mae: 0.0186\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.8993e-04 - mae: 0.0185\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.8583e-04 - mae: 0.0184\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.8174e-04 - mae: 0.0183\n",
      "Epoch 602/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.7771e-04 - mae: 0.0183\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.7371e-04 - mae: 0.0182\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.6974e-04 - mae: 0.0181\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.6579e-04 - mae: 0.0180\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.6189e-04 - mae: 0.0180\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.5802e-04 - mae: 0.0179\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.5419e-04 - mae: 0.0178\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.5037e-04 - mae: 0.0177\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.4660e-04 - mae: 0.0177\n",
      "Epoch 611/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.4286e-04 - mae: 0.0176\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.3915e-04 - mae: 0.0175\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.3548e-04 - mae: 0.0174\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.3182e-04 - mae: 0.0174\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.2820e-04 - mae: 0.0173\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.2461e-04 - mae: 0.0172\n",
      "Epoch 617/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.2105e-04 - mae: 0.0171\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.1753e-04 - mae: 0.0171\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.1402e-04 - mae: 0.0170\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.1056e-04 - mae: 0.0169\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.0711e-04 - mae: 0.0169\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.0369e-04 - mae: 0.0168\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.0032e-04 - mae: 0.0167\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.9696e-04 - mae: 0.0166\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.9363e-04 - mae: 0.0166\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.9033e-04 - mae: 0.0165\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8706e-04 - mae: 0.0164\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8381e-04 - mae: 0.0164\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8060e-04 - mae: 0.0163\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7741e-04 - mae: 0.0162\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7425e-04 - mae: 0.0162\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.7112e-04 - mae: 0.0161\n",
      "Epoch 633/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.6799e-04 - mae: 0.0160\n",
      "Epoch 634/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.6492e-04 - mae: 0.0160\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.6185e-04 - mae: 0.0159\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.5882e-04 - mae: 0.0158\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.5582e-04 - mae: 0.0158\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.5284e-04 - mae: 0.0157\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.4988e-04 - mae: 0.0156\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.4694e-04 - mae: 0.0156\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.4405e-04 - mae: 0.0155\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.4116e-04 - mae: 0.0154\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.3830e-04 - mae: 0.0154\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.3546e-04 - mae: 0.0153\n",
      "Epoch 645/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.3264e-04 - mae: 0.0152\n",
      "Epoch 646/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.2986e-04 - mae: 0.0152\n",
      "Epoch 647/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.2708e-04 - mae: 0.0151\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.2435e-04 - mae: 0.0150\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.2164e-04 - mae: 0.0150\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.1893e-04 - mae: 0.0149\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.1626e-04 - mae: 0.0149\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.1361e-04 - mae: 0.0148\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.1099e-04 - mae: 0.0147\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.0838e-04 - mae: 0.0147\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.0580e-04 - mae: 0.0146\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0324e-04 - mae: 0.0145\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.0069e-04 - mae: 0.0145\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.9817e-04 - mae: 0.0144\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.9567e-04 - mae: 0.0144\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.9319e-04 - mae: 0.0143\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.9073e-04 - mae: 0.0142\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.8829e-04 - mae: 0.0142\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.8588e-04 - mae: 0.0141\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.8349e-04 - mae: 0.0141\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.8112e-04 - mae: 0.0140\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.7875e-04 - mae: 0.0139\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.7642e-04 - mae: 0.0139\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.7410e-04 - mae: 0.0138\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.7181e-04 - mae: 0.0138\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.6953e-04 - mae: 0.0137\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.6727e-04 - mae: 0.0137\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.6503e-04 - mae: 0.0136\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.6280e-04 - mae: 0.0135\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.6060e-04 - mae: 0.0135\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.5842e-04 - mae: 0.0134\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.5626e-04 - mae: 0.0134\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.5411e-04 - mae: 0.0133\n",
      "Epoch 678/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.5197e-04 - mae: 0.0133\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.4987e-04 - mae: 0.0132\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.4777e-04 - mae: 0.0131\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.4570e-04 - mae: 0.0131\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.4364e-04 - mae: 0.0130\n",
      "Epoch 683/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.4159e-04 - mae: 0.0130\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.3957e-04 - mae: 0.0129\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.3755e-04 - mae: 0.0129\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3557e-04 - mae: 0.0128\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3360e-04 - mae: 0.0128\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3163e-04 - mae: 0.0127\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2970e-04 - mae: 0.0127\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2777e-04 - mae: 0.0126\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2587e-04 - mae: 0.0126\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2397e-04 - mae: 0.0125\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2209e-04 - mae: 0.0124\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2022e-04 - mae: 0.0124\n",
      "Epoch 695/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1838e-04 - mae: 0.0123\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1656e-04 - mae: 0.0123\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1474e-04 - mae: 0.0122\n",
      "Epoch 698/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1294e-04 - mae: 0.0122\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1115e-04 - mae: 0.0121\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0938e-04 - mae: 0.0121\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0763e-04 - mae: 0.0120\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0589e-04 - mae: 0.0120\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0416e-04 - mae: 0.0119\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0245e-04 - mae: 0.0119\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0075e-04 - mae: 0.0118\n",
      "Epoch 706/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9907e-04 - mae: 0.0118\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9740e-04 - mae: 0.0117\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9575e-04 - mae: 0.0117\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9410e-04 - mae: 0.0116\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9249e-04 - mae: 0.0116\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9087e-04 - mae: 0.0115\n",
      "Epoch 712/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8926e-04 - mae: 0.0115\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8768e-04 - mae: 0.0114\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8610e-04 - mae: 0.0114\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8455e-04 - mae: 0.0113\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8300e-04 - mae: 0.0113\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8147e-04 - mae: 0.0113\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7995e-04 - mae: 0.0112\n",
      "Epoch 719/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7844e-04 - mae: 0.0112\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7695e-04 - mae: 0.0111\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7546e-04 - mae: 0.0111\n",
      "Epoch 722/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7399e-04 - mae: 0.0110\n",
      "Epoch 723/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7253e-04 - mae: 0.0110\n",
      "Epoch 724/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7109e-04 - mae: 0.0109\n",
      "Epoch 725/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.6965e-04 - mae: 0.0109\n",
      "Epoch 726/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.6823e-04 - mae: 0.0108\n",
      "Epoch 727/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.6681e-04 - mae: 0.0108\n",
      "Epoch 728/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.6542e-04 - mae: 0.0107\n",
      "Epoch 729/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.6404e-04 - mae: 0.0107\n",
      "Epoch 730/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.6266e-04 - mae: 0.0107\n",
      "Epoch 731/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.6130e-04 - mae: 0.0106\n",
      "Epoch 732/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.5995e-04 - mae: 0.0106\n",
      "Epoch 733/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5861e-04 - mae: 0.0105\n",
      "Epoch 734/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5727e-04 - mae: 0.0105\n",
      "Epoch 735/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5596e-04 - mae: 0.0104\n",
      "Epoch 736/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5465e-04 - mae: 0.0104\n",
      "Epoch 737/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.5335e-04 - mae: 0.0103\n",
      "Epoch 738/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.5206e-04 - mae: 0.0103\n",
      "Epoch 739/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.5079e-04 - mae: 0.0103\n",
      "Epoch 740/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4952e-04 - mae: 0.0102\n",
      "Epoch 741/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4827e-04 - mae: 0.0102\n",
      "Epoch 742/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4703e-04 - mae: 0.0101\n",
      "Epoch 743/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4580e-04 - mae: 0.0101\n",
      "Epoch 744/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4457e-04 - mae: 0.0100\n",
      "Epoch 745/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4337e-04 - mae: 0.0100\n",
      "Epoch 746/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4217e-04 - mae: 0.0100\n",
      "Epoch 747/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4097e-04 - mae: 0.0099\n",
      "Epoch 748/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3980e-04 - mae: 0.0099\n",
      "Epoch 749/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3862e-04 - mae: 0.0098\n",
      "Epoch 750/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3745e-04 - mae: 0.0098\n",
      "Epoch 751/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3631e-04 - mae: 0.0098\n",
      "Epoch 752/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3516e-04 - mae: 0.0097\n",
      "Epoch 753/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3404e-04 - mae: 0.0097\n",
      "Epoch 754/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3291e-04 - mae: 0.0096\n",
      "Epoch 755/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3179e-04 - mae: 0.0096\n",
      "Epoch 756/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3069e-04 - mae: 0.0096\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2959e-04 - mae: 0.0095\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2851e-04 - mae: 0.0095\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2743e-04 - mae: 0.0094\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2636e-04 - mae: 0.0094\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2531e-04 - mae: 0.0094\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2425e-04 - mae: 0.0093\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2321e-04 - mae: 0.0093\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2218e-04 - mae: 0.0092\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2115e-04 - mae: 0.0092\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2014e-04 - mae: 0.0092\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1913e-04 - mae: 0.0091\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1814e-04 - mae: 0.0091\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1714e-04 - mae: 0.0090\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1616e-04 - mae: 0.0090\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1519e-04 - mae: 0.0090\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1422e-04 - mae: 0.0089\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1327e-04 - mae: 0.0089\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1232e-04 - mae: 0.0089\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1138e-04 - mae: 0.0088\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1044e-04 - mae: 0.0088\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0952e-04 - mae: 0.0087\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0859e-04 - mae: 0.0087\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0768e-04 - mae: 0.0087\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0678e-04 - mae: 0.0086\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0589e-04 - mae: 0.0086\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0500e-04 - mae: 0.0086\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0413e-04 - mae: 0.0085\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0325e-04 - mae: 0.0085\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0238e-04 - mae: 0.0085\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0153e-04 - mae: 0.0084\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0068e-04 - mae: 0.0084\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.9830e-05 - mae: 0.0083\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.8995e-05 - mae: 0.0083\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.8168e-05 - mae: 0.0083\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.7342e-05 - mae: 0.0082\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6524e-05 - mae: 0.0082\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.5717e-05 - mae: 0.0082\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.4913e-05 - mae: 0.0081\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.4118e-05 - mae: 0.0081\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.3331e-05 - mae: 0.0081\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2550e-05 - mae: 0.0080\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.1777e-05 - mae: 0.0080\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.1005e-05 - mae: 0.0080\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.0244e-05 - mae: 0.0079\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.9490e-05 - mae: 0.0079\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.8738e-05 - mae: 0.0079\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.7987e-05 - mae: 0.0078\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.7252e-05 - mae: 0.0078\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.6524e-05 - mae: 0.0078\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.5802e-05 - mae: 0.0077\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.5078e-05 - mae: 0.0077\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.4366e-05 - mae: 0.0077\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.3656e-05 - mae: 0.0076\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.2956e-05 - mae: 0.0076\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.2261e-05 - mae: 0.0076\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.1567e-05 - mae: 0.0075\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0883e-05 - mae: 0.0075\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0210e-05 - mae: 0.0075\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.9542e-05 - mae: 0.0075\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.8873e-05 - mae: 0.0074\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.8207e-05 - mae: 0.0074\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.7550e-05 - mae: 0.0074\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.6901e-05 - mae: 0.0073\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.6259e-05 - mae: 0.0073\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.5623e-05 - mae: 0.0073\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.4986e-05 - mae: 0.0072\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.4361e-05 - mae: 0.0072\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.3737e-05 - mae: 0.0072\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.3116e-05 - mae: 0.0071\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.2506e-05 - mae: 0.0071\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.1897e-05 - mae: 0.0071\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.1290e-05 - mae: 0.0071\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0701e-05 - mae: 0.0070\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.0108e-05 - mae: 0.0070\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9512e-05 - mae: 0.0070\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.8934e-05 - mae: 0.0069\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.8359e-05 - mae: 0.0069\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.7782e-05 - mae: 0.0069\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.7214e-05 - mae: 0.0068\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.6650e-05 - mae: 0.0068\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.6093e-05 - mae: 0.0068\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.5536e-05 - mae: 0.0068\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.4991e-05 - mae: 0.0067\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.4444e-05 - mae: 0.0067\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.3904e-05 - mae: 0.0067\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.3372e-05 - mae: 0.0067\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.2835e-05 - mae: 0.0066\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.2311e-05 - mae: 0.0066\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.1794e-05 - mae: 0.0066\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.1272e-05 - mae: 0.0065\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.0758e-05 - mae: 0.0065\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.0248e-05 - mae: 0.0065\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.9744e-05 - mae: 0.0065\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.9241e-05 - mae: 0.0064\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.8751e-05 - mae: 0.0064\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.8254e-05 - mae: 0.0064\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.7764e-05 - mae: 0.0063\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.7279e-05 - mae: 0.0063\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.6803e-05 - mae: 0.0063\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.6325e-05 - mae: 0.0063\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.5855e-05 - mae: 0.0062\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.5388e-05 - mae: 0.0062\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.4921e-05 - mae: 0.0062\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.4459e-05 - mae: 0.0062\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.4003e-05 - mae: 0.0061\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3550e-05 - mae: 0.0061\n",
      "Epoch 863/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3105e-05 - mae: 0.0061\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.2659e-05 - mae: 0.0061\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.2217e-05 - mae: 0.0060\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.1782e-05 - mae: 0.0060\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.1345e-05 - mae: 0.0060\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.0915e-05 - mae: 0.0060\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.0490e-05 - mae: 0.0059\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.0068e-05 - mae: 0.0059\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.9642e-05 - mae: 0.0059\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.9230e-05 - mae: 0.0059\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.8816e-05 - mae: 0.0058\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.8406e-05 - mae: 0.0058\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.7998e-05 - mae: 0.0058\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7597e-05 - mae: 0.0058\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.7200e-05 - mae: 0.0057\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.6804e-05 - mae: 0.0057\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.6408e-05 - mae: 0.0057\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.6024e-05 - mae: 0.0057\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.5639e-05 - mae: 0.0056\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.5256e-05 - mae: 0.0056\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.4877e-05 - mae: 0.0056\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.4499e-05 - mae: 0.0056\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.4127e-05 - mae: 0.0055\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.3753e-05 - mae: 0.0055\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.3390e-05 - mae: 0.0055\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.3026e-05 - mae: 0.0055\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.2664e-05 - mae: 0.0055\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.2305e-05 - mae: 0.0054\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.1953e-05 - mae: 0.0054\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.1601e-05 - mae: 0.0054\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.1255e-05 - mae: 0.0054\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.0905e-05 - mae: 0.0053\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.0564e-05 - mae: 0.0053\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.0227e-05 - mae: 0.0053\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.9888e-05 - mae: 0.0053\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.9551e-05 - mae: 0.0053\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.9224e-05 - mae: 0.0052\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8894e-05 - mae: 0.0052\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.8569e-05 - mae: 0.0052\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.8244e-05 - mae: 0.0052\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.7928e-05 - mae: 0.0051\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7607e-05 - mae: 0.0051\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.7292e-05 - mae: 0.0051\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.6978e-05 - mae: 0.0051\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.6668e-05 - mae: 0.0051\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.6362e-05 - mae: 0.0050\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.6056e-05 - mae: 0.0050\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.5756e-05 - mae: 0.0050\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.5453e-05 - mae: 0.0050\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.5159e-05 - mae: 0.0050\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.4862e-05 - mae: 0.0049\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.4571e-05 - mae: 0.0049\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.4281e-05 - mae: 0.0049\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.3994e-05 - mae: 0.0049\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.3711e-05 - mae: 0.0049\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.3425e-05 - mae: 0.0048\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.3148e-05 - mae: 0.0048\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.2869e-05 - mae: 0.0048\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.2593e-05 - mae: 0.0048\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.2320e-05 - mae: 0.0047\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.2052e-05 - mae: 0.0047\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.1778e-05 - mae: 0.0047\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.1513e-05 - mae: 0.0047\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.1250e-05 - mae: 0.0047\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0990e-05 - mae: 0.0047\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.0730e-05 - mae: 0.0046\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.0468e-05 - mae: 0.0046\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.0217e-05 - mae: 0.0046\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.9963e-05 - mae: 0.0046\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.9713e-05 - mae: 0.0046\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.9464e-05 - mae: 0.0045\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.9214e-05 - mae: 0.0045\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.8970e-05 - mae: 0.0045\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.8729e-05 - mae: 0.0045\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.8485e-05 - mae: 0.0045\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.8250e-05 - mae: 0.0044\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.8011e-05 - mae: 0.0044\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.7778e-05 - mae: 0.0044\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.7545e-05 - mae: 0.0044\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.7313e-05 - mae: 0.0044\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.7084e-05 - mae: 0.0043\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.6858e-05 - mae: 0.0043\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.6631e-05 - mae: 0.0043\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.6409e-05 - mae: 0.0043\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.6185e-05 - mae: 0.0043\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.5968e-05 - mae: 0.0043\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.5751e-05 - mae: 0.0042\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.5535e-05 - mae: 0.0042\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.5321e-05 - mae: 0.0042\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.5108e-05 - mae: 0.0042\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.4899e-05 - mae: 0.0042\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4690e-05 - mae: 0.0042\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.4482e-05 - mae: 0.0041\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.4278e-05 - mae: 0.0041\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4073e-05 - mae: 0.0041\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3875e-05 - mae: 0.0041\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3673e-05 - mae: 0.0041\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3473e-05 - mae: 0.0040\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.3278e-05 - mae: 0.0040\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.3082e-05 - mae: 0.0040\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2889e-05 - mae: 0.0040\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2695e-05 - mae: 0.0040\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2508e-05 - mae: 0.0040\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.2319e-05 - mae: 0.0039\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2130e-05 - mae: 0.0039\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1947e-05 - mae: 0.0039\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1763e-05 - mae: 0.0039\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1579e-05 - mae: 0.0039\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1398e-05 - mae: 0.0039\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1220e-05 - mae: 0.0038\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1041e-05 - mae: 0.0038\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.0864e-05 - mae: 0.0038\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0692e-05 - mae: 0.0038\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0517e-05 - mae: 0.0038\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0347e-05 - mae: 0.0038\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0174e-05 - mae: 0.0038\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0005e-05 - mae: 0.0037\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9837e-05 - mae: 0.0037\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9672e-05 - mae: 0.0037\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9506e-05 - mae: 0.0037\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9344e-05 - mae: 0.0037\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9180e-05 - mae: 0.0037\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9021e-05 - mae: 0.0036\n",
      "Epoch 986/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8862e-05 - mae: 0.0036\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8703e-05 - mae: 0.0036\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8547e-05 - mae: 0.0036\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8390e-05 - mae: 0.0036\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8239e-05 - mae: 0.0036\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8084e-05 - mae: 0.0036\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7933e-05 - mae: 0.0035\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7781e-05 - mae: 0.0035\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7630e-05 - mae: 0.0035\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7483e-05 - mae: 0.0035\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7338e-05 - mae: 0.0035\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7193e-05 - mae: 0.0035\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7050e-05 - mae: 0.0034\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6905e-05 - mae: 0.0034\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6764e-05 - mae: 0.0034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x281cfd38460>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습하기\n",
    "model.fit(x_train, y_train, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a5315-5926-491a-a9e0-25f963eb1bad",
   "metadata": {},
   "source": [
    "학습이 완료된 모델의 가중치를 확인합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a01f1c8-ad40-4387-bd66-07b4eb61cc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[2.001265]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.99119264], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# 모델 가중치 확인하기\n",
    "print(model.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69422390-8e42-47c5-a184-8bddcf6b0152",
   "metadata": {},
   "source": [
    "모델 내 각 레이어의 가중치와 편향을 개별적으로 학인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "405b13e7-88fe-4a9a-83b9-22febff4e9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : [[2.001265]]\n",
      "bias : [0.99119264]\n"
     ]
    }
   ],
   "source": [
    "# 모델 레이어의 가중치 출력하기\n",
    "print(f'weight : {model.layers[0].weights[0].numpy()}')\n",
    "print(f'bias : {model.layers[0].bias.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94accb68-3b4c-4f76-881a-b23daf0e24c6",
   "metadata": {},
   "source": [
    "학습이 완료된 모델을 사용하고 예측하고 결과를 확인해 봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a00affe8-5b90-4aa0-bbb7-12c723824ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 95ms/step\n",
      "[[23.005108]\n",
      " [25.006372]\n",
      " [27.007637]]\n"
     ]
    }
   ],
   "source": [
    "# 학습 완료된 모델 사용하여 예측하기\n",
    "print(model.predict([[11],[12],[13]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4c5a8-0423-4709-b236-0532966a7a83",
   "metadata": {},
   "source": [
    "학습이 완료된 모델의 가중치와 편향을 대입하여 선형함수로 표현하면 다음과 같습니다. \n",
    "\n",
    "$$y=2.001265*x + 0.99119264$$\n",
    "모델의 출력은 입력 데이터에 2를 곱하고 1을 더한 값과 비슷하게 나온다는 것을 알 수 있습니다. help 함수로 클래스(객체)와 메소드의 사용법을 출력할 수 있으므로, 자세한 사용법을 help 함수로 확인합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b5148-050e-4112-a4bc-3ea8f3673d8f",
   "metadata": {},
   "source": [
    "다음 예제는 hlep를 사용하여 확인한 fit() 메소드 내용입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77a34d90-d0f8-4da5-b471-a155999f7781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module keras.src.engine.training:\n",
      "\n",
      "fit(x=None, y=None, batch_size=None, epochs=1, verbose='auto', callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) method of keras.src.engine.sequential.Sequential instance\n",
      "    Trains the model for a fixed number of epochs (dataset iterations).\n",
      "    \n",
      "    Args:\n",
      "        x: Input data. It could be:\n",
      "          - A Numpy array (or array-like), or a list of arrays\n",
      "            (in case the model has multiple inputs).\n",
      "          - A TensorFlow tensor, or a list of tensors\n",
      "            (in case the model has multiple inputs).\n",
      "          - A dict mapping input names to the corresponding array/tensors,\n",
      "            if the model has named inputs.\n",
      "          - A `tf.data` dataset. Should return a tuple\n",
      "            of either `(inputs, targets)` or\n",
      "            `(inputs, targets, sample_weights)`.\n",
      "          - A generator or `keras.utils.Sequence` returning `(inputs,\n",
      "            targets)` or `(inputs, targets, sample_weights)`.\n",
      "          - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n",
      "            callable that takes a single argument of type\n",
      "            `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n",
      "            `DatasetCreator` should be used when users prefer to specify the\n",
      "            per-replica batching and sharding logic for the `Dataset`.\n",
      "            See `tf.keras.utils.experimental.DatasetCreator` doc for more\n",
      "            information.\n",
      "          A more detailed description of unpacking behavior for iterator\n",
      "          types (Dataset, generator, Sequence) is given below. If these\n",
      "          include `sample_weights` as a third component, note that sample\n",
      "          weighting applies to the `weighted_metrics` argument but not the\n",
      "          `metrics` argument in `compile()`. If using\n",
      "          `tf.distribute.experimental.ParameterServerStrategy`, only\n",
      "          `DatasetCreator` type is supported for `x`.\n",
      "        y: Target data. Like the input data `x`,\n",
      "          it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      "          It should be consistent with `x` (you cannot have Numpy inputs and\n",
      "          tensor targets, or inversely). If `x` is a dataset, generator,\n",
      "          or `keras.utils.Sequence` instance, `y` should\n",
      "          not be specified (since targets will be obtained from `x`).\n",
      "        batch_size: Integer or `None`.\n",
      "            Number of samples per gradient update.\n",
      "            If unspecified, `batch_size` will default to 32.\n",
      "            Do not specify the `batch_size` if your data is in the\n",
      "            form of datasets, generators, or `keras.utils.Sequence`\n",
      "            instances (since they generate batches).\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire `x` and `y`\n",
      "            data provided\n",
      "            (unless the `steps_per_epoch` flag is set to\n",
      "            something other than None).\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: 'auto', 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "            'auto' becomes 1 for most cases, but 2 when used with\n",
      "            `ParameterServerStrategy`. Note that the progress bar is not\n",
      "            particularly useful when logged to a file, so verbose=2 is\n",
      "            recommended when not running interactively (eg, in a production\n",
      "            environment). Defaults to 'auto'.\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See `tf.keras.callbacks`. Note\n",
      "            `tf.keras.callbacks.ProgbarLogger` and\n",
      "            `tf.keras.callbacks.History` callbacks are created automatically\n",
      "            and need not be passed into `model.fit`.\n",
      "            `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
      "            `verbose` argument to `model.fit`.\n",
      "            Callbacks with batch-level calls are currently unsupported with\n",
      "            `tf.distribute.experimental.ParameterServerStrategy`, and users\n",
      "            are advised to implement epoch-level calls instead with an\n",
      "            appropriate `steps_per_epoch` value.\n",
      "        validation_split: Float between 0 and 1.\n",
      "            Fraction of the training data to be used as validation data.\n",
      "            The model will set apart this fraction of the training data,\n",
      "            will not train on it, and will evaluate\n",
      "            the loss and any model metrics\n",
      "            on this data at the end of each epoch.\n",
      "            The validation data is selected from the last samples\n",
      "            in the `x` and `y` data provided, before shuffling. This\n",
      "            argument is not supported when `x` is a dataset, generator or\n",
      "            `keras.utils.Sequence` instance.\n",
      "            If both `validation_data` and `validation_split` are provided,\n",
      "            `validation_data` will override `validation_split`.\n",
      "            `validation_split` is not yet supported with\n",
      "            `tf.distribute.experimental.ParameterServerStrategy`.\n",
      "        validation_data: Data on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data. Thus, note the fact\n",
      "            that the validation loss of data provided using\n",
      "            `validation_split` or `validation_data` is not affected by\n",
      "            regularization layers like noise and dropout.\n",
      "            `validation_data` will override `validation_split`.\n",
      "            `validation_data` could be:\n",
      "              - A tuple `(x_val, y_val)` of Numpy arrays or tensors.\n",
      "              - A tuple `(x_val, y_val, val_sample_weights)` of NumPy\n",
      "                arrays.\n",
      "              - A `tf.data.Dataset`.\n",
      "              - A Python generator or `keras.utils.Sequence` returning\n",
      "              `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n",
      "            `validation_data` is not yet supported with\n",
      "            `tf.distribute.experimental.ParameterServerStrategy`.\n",
      "        shuffle: Boolean (whether to shuffle the training data\n",
      "            before each epoch) or str (for 'batch'). This argument is\n",
      "            ignored when `x` is a generator or an object of tf.data.Dataset.\n",
      "            'batch' is a special option for dealing\n",
      "            with the limitations of HDF5 data; it shuffles in batch-sized\n",
      "            chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only).\n",
      "            This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples from\n",
      "            an under-represented class. When `class_weight` is specified\n",
      "            and targets have a rank of 2 or greater, either `y` must be\n",
      "            one-hot encoded, or an explicit final dimension of `1` must\n",
      "            be included for sparse class labels.\n",
      "        sample_weight: Optional Numpy array of weights for\n",
      "            the training samples, used for weighting the loss function\n",
      "            (during training only). You can either pass a flat (1D)\n",
      "            Numpy array with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples),\n",
      "            or in the case of temporal data,\n",
      "            you can pass a 2D array with shape\n",
      "            `(samples, sequence_length)`,\n",
      "            to apply a different weight to every timestep of every sample.\n",
      "            This argument is not supported when `x` is a dataset, generator,\n",
      "            or `keras.utils.Sequence` instance, instead provide the\n",
      "            sample_weights as the third element of `x`.\n",
      "            Note that sample weighting does not apply to metrics specified\n",
      "            via the `metrics` argument in `compile()`. To apply sample\n",
      "            weighting to your metrics, you can specify them via the\n",
      "            `weighted_metrics` in `compile()` instead.\n",
      "        initial_epoch: Integer.\n",
      "            Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "        steps_per_epoch: Integer or `None`.\n",
      "            Total number of steps (batches of samples)\n",
      "            before declaring one epoch finished and starting the\n",
      "            next epoch. When training with input tensors such as\n",
      "            TensorFlow data tensors, the default `None` is equal to\n",
      "            the number of samples in your dataset divided by\n",
      "            the batch size, or 1 if that cannot be determined. If x is a\n",
      "            `tf.data` dataset, and 'steps_per_epoch'\n",
      "            is None, the epoch will run until the input dataset is\n",
      "            exhausted.  When passing an infinitely repeating dataset, you\n",
      "            must specify the `steps_per_epoch` argument. If\n",
      "            `steps_per_epoch=-1` the training will run indefinitely with an\n",
      "            infinitely repeating dataset.  This argument is not supported\n",
      "            with array inputs.\n",
      "            When using `tf.distribute.experimental.ParameterServerStrategy`:\n",
      "              * `steps_per_epoch=None` is not supported.\n",
      "        validation_steps: Only relevant if `validation_data` is provided and\n",
      "            is a `tf.data` dataset. Total number of steps (batches of\n",
      "            samples) to draw before stopping when performing validation\n",
      "            at the end of every epoch. If 'validation_steps' is None,\n",
      "            validation will run until the `validation_data` dataset is\n",
      "            exhausted. In the case of an infinitely repeated dataset, it\n",
      "            will run into an infinite loop. If 'validation_steps' is\n",
      "            specified and only part of the dataset will be consumed, the\n",
      "            evaluation will start from the beginning of the dataset at each\n",
      "            epoch. This ensures that the same validation samples are used\n",
      "            every time.\n",
      "        validation_batch_size: Integer or `None`.\n",
      "            Number of samples per validation batch.\n",
      "            If unspecified, will default to `batch_size`.\n",
      "            Do not specify the `validation_batch_size` if your data is in\n",
      "            the form of datasets, generators, or `keras.utils.Sequence`\n",
      "            instances (since they generate batches).\n",
      "        validation_freq: Only relevant if validation data is provided.\n",
      "          Integer or `collections.abc.Container` instance (e.g. list, tuple,\n",
      "          etc.).  If an integer, specifies how many training epochs to run\n",
      "          before a new validation run is performed, e.g. `validation_freq=2`\n",
      "          runs validation every 2 epochs. If a Container, specifies the\n",
      "          epochs on which to run validation, e.g.\n",
      "          `validation_freq=[1, 2, 10]` runs validation at the end of the\n",
      "          1st, 2nd, and 10th epochs.\n",
      "        max_queue_size: Integer. Used for generator or\n",
      "          `keras.utils.Sequence` input only. Maximum size for the generator\n",
      "          queue.  If unspecified, `max_queue_size` will default to 10.\n",
      "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      "            only. Maximum number of processes to spin up\n",
      "            when using process-based threading. If unspecified, `workers`\n",
      "            will default to 1.\n",
      "        use_multiprocessing: Boolean. Used for generator or\n",
      "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
      "            threading. If unspecified, `use_multiprocessing` will default to\n",
      "            `False`. Note that because this implementation relies on\n",
      "            multiprocessing, you should not pass non-picklable arguments to\n",
      "            the generator as they can't be passed easily to children\n",
      "            processes.\n",
      "    \n",
      "    Unpacking behavior for iterator-like inputs:\n",
      "        A common pattern is to pass a tf.data.Dataset, generator, or\n",
      "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
      "      yield not only features (x) but optionally targets (y) and sample\n",
      "      weights.  Keras requires that the output of such iterator-likes be\n",
      "      unambiguous. The iterator should return a tuple of length 1, 2, or 3,\n",
      "      where the optional second and third elements will be used for y and\n",
      "      sample_weight respectively. Any other type provided will be wrapped in\n",
      "      a length one tuple, effectively treating everything as 'x'. When\n",
      "      yielding dicts, they should still adhere to the top-level tuple\n",
      "      structure.\n",
      "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      "      features, targets, and weights from the keys of a single dict.\n",
      "        A notable unsupported data type is the namedtuple. The reason is\n",
      "      that it behaves like both an ordered datatype (tuple) and a mapping\n",
      "      datatype (dict). So given a namedtuple of the form:\n",
      "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      "      it is ambiguous whether to reverse the order of the elements when\n",
      "      interpreting the value. Even worse is a tuple of the form:\n",
      "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      "      where it is unclear if the tuple was intended to be unpacked into x,\n",
      "      y, and sample_weight or passed through as a single element to `x`. As\n",
      "      a result the data processing code will simply raise a ValueError if it\n",
      "      encounters a namedtuple. (Along with instructions to remedy the\n",
      "      issue.)\n",
      "    \n",
      "    Returns:\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "    \n",
      "    Raises:\n",
      "        RuntimeError: 1. If the model was never compiled or,\n",
      "        2. If `model.fit` is  wrapped in `tf.function`.\n",
      "    \n",
      "        ValueError: In case of mismatch between the provided input data\n",
      "            and what the model expects or when the input data is empty.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 클래스와 메소드 사용법 확인하기\n",
    "help(model.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114a480-4fe9-44d6-bcfe-d1dc1b0c70f8",
   "metadata": {},
   "source": [
    "## 4. 심층신경망으로 항공사 고객 만족 분류 모델 구현 실습하기\n",
    "심층신경망으로 항공사 이용 고객의 만족 여부를 분류하는 모델을 구현해 봅니다. \n",
    "\n",
    "### 1) 데이터 불러오기 및 확인하기\n",
    "데이터는 항공사 고객 만족 예측 데이터(Airline Customer Satisfaction)의 ‘Inveistico_Airline.csv’를 사용합니다.<br>\n",
    "- 데이터 출처：https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction<br>\n",
    "- 데이터명：invisitico_Airline.csv\n",
    "\n",
    "**■ 데이터 설명**<br>\n",
    "항공사 고객 12만 9,880명의 설문 결과와 인구통계/개인정보로 구성되어 있으며, 데이터에는 총 23개의 속성이 있습니다. 먼저 9개의 변수를 확인해 봅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662c0137-b316-4b54-8fc2-f41107351a8a",
   "metadata": {},
   "source": [
    "<div style=\"display:table; border-collapse:collapse; width:80%; text-align:center;\">\n",
    "    <div style=\"display:table-row; background-color:#d9e2f3; font-weight:bold\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">칼럼명</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">설명</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">비고</div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Satisfaction</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">고객 만족 여부(Y)</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Satisfied : 만족<br>Dissatisfied : 불만족</div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Gender</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">성별</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Female : 여성<br>Male : 남성</div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Customer Type</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">고객 유형</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Loyal Customer : 충성 고객<br>Disloyal Customer : 비충성 고객</div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Age</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">나이</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">리뷰를 받은 고객의 나이</div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Type of Travel</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">여행 유형</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Business travel : 비즈니스 여행<br>Personal Travel : 개인 여행</div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Class</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">좌석 등급</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Business : 비즈니스<br>Eco : 이코노믹<br>Eco Plus: 이코노믹 플러스</div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Flight Distance</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">비행거리</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">&nbsp;</div>\n",
    "    </div>    \n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Departure Delay in Minutes</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">출발 지연 시간(분)</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">&nbsp;</div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Arrival Delay in Minutes</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">도착 지연 시간(분)</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">&nbsp;</div>\n",
    "    </div>\n",
    "</div>\n",
    "다음 14개의 변수는 고객의 만족도 항목으로 0에서 5 사이의 척도 점수를 매깁니다. \n",
    "<div style=\"display:table; border-collapse:collapse; width:40%; text-align:center;\">\n",
    "    <div style=\"display:table-row; background-color:#d9e2f3; font-weight:bold\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">칼럼명</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px;\">설명</div>\n",
    "    </div>\n",
    "    <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Seat comfort</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">좌석의 편안함</div>\n",
    "    </div>\n",
    "        <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Departure/Arrival time convenient</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">편리한 출발/도착 시간</div>\n",
    "    </div>\n",
    "        <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Food and drink</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">음식과 음료</div>\n",
    "    </div>\n",
    "     <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Gate location</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">게이트 위치</div>\n",
    "    </div>\n",
    "        <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Inflight entertainment</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">기내 와이파이 서비스</div>\n",
    "    </div>\n",
    "        <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Online support</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">기내 엔터테인먼트</div>\n",
    "    </div>\n",
    "        <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Online support</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">온라인 지원</div>\n",
    "    </div>\n",
    "        <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Ease of Online booking</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">온라인 예약의 용이성</div>\n",
    "    </div>\n",
    "        <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">On-board service</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">온보드 서비스</div>\n",
    "    </div>\n",
    "        <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Leg room service</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">레그룸 서비스</div>\n",
    "    </div>\n",
    "        <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Baggage handling</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">수화물 처리</div>\n",
    "    </div>\n",
    "        <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Checkin service</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">체크인 서비스</div>\n",
    "    </div>\n",
    "        <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Cleanliness</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">청결</div>\n",
    "    </div>\n",
    "        <div style=\"display:table-row;\">\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">Online boarding</div>\n",
    "        <div style=\"display:table-cell; border:1px solid black; padding:5px; text-align:left;\">온라인 탑승</div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dab04684-ce88-434f-b935-02a003862bfa",
   "metadata": {},
   "source": [
    "#261"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b2176f-0d62-4c6d-9573-226897e88d45",
   "metadata": {},
   "source": [
    "필요한 라이브러리를 불러옵니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "164c52b8-0410-4e2d-8ff1-2a608387a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296b405-f305-4917-b265-a682cca3309c",
   "metadata": {},
   "source": [
    "Pandas library의 read_csv() 메소드를 사용해, csv 파일에서 데이터를 불러오고 데이터 프레임(DataFrame)으로 저장합니다.<br>\n",
    "infor() 메소드로 데이터프레임의 정보를 확인합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bdaf6f7-3134-4a45-b384-261d3fe58c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 129880 entries, 0 to 129879\n",
      "Data columns (total 23 columns):\n",
      " #   Column                             Non-Null Count   Dtype  \n",
      "---  ------                             --------------   -----  \n",
      " 0   satisfaction                       129880 non-null  object \n",
      " 1   Gender                             129880 non-null  object \n",
      " 2   Customer Type                      129880 non-null  object \n",
      " 3   Age                                129880 non-null  int64  \n",
      " 4   Type of Travel                     129880 non-null  object \n",
      " 5   Class                              129880 non-null  object \n",
      " 6   Flight Distance                    129880 non-null  int64  \n",
      " 7   Seat comfort                       129880 non-null  int64  \n",
      " 8   Departure/Arrival time convenient  129880 non-null  int64  \n",
      " 9   Food and drink                     129880 non-null  int64  \n",
      " 10  Gate location                      129880 non-null  int64  \n",
      " 11  Inflight wifi service              129880 non-null  int64  \n",
      " 12  Inflight entertainment             129880 non-null  int64  \n",
      " 13  Online support                     129880 non-null  int64  \n",
      " 14  Ease of Online booking             129880 non-null  int64  \n",
      " 15  On-board service                   129880 non-null  int64  \n",
      " 16  Leg room service                   129880 non-null  int64  \n",
      " 17  Baggage handling                   129880 non-null  int64  \n",
      " 18  Checkin service                    129880 non-null  int64  \n",
      " 19  Cleanliness                        129880 non-null  int64  \n",
      " 20  Online boarding                    129880 non-null  int64  \n",
      " 21  Departure Delay in Minutes         129880 non-null  int64  \n",
      " 22  Arrival Delay in Minutes           129487 non-null  float64\n",
      "dtypes: float64(1), int64(17), object(5)\n",
      "memory usage: 22.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# csv 파일에서 데이터를 로드해서 데이터프레임으로 저장하기\n",
    "df = pd.read_csv('Invistico_Airline.csv')\n",
    "\n",
    "# 데이터프레임 정보 확인하기\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae188b4-aebb-40d2-a790-398cc2608f1f",
   "metadata": {},
   "source": [
    "#262"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3e4d5-b417-4c14-b95b-66488cb966a5",
   "metadata": {},
   "source": [
    "데이터를 살펴봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "360eaf6c-aaa3-4af7-b44f-4ed31d9762d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Customer Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Type of Travel</th>\n",
       "      <th>Class</th>\n",
       "      <th>Flight Distance</th>\n",
       "      <th>Seat comfort</th>\n",
       "      <th>Departure/Arrival time convenient</th>\n",
       "      <th>Food and drink</th>\n",
       "      <th>...</th>\n",
       "      <th>Online support</th>\n",
       "      <th>Ease of Online booking</th>\n",
       "      <th>On-board service</th>\n",
       "      <th>Leg room service</th>\n",
       "      <th>Baggage handling</th>\n",
       "      <th>Checkin service</th>\n",
       "      <th>Cleanliness</th>\n",
       "      <th>Online boarding</th>\n",
       "      <th>Departure Delay in Minutes</th>\n",
       "      <th>Arrival Delay in Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>satisfied</td>\n",
       "      <td>Female</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>65</td>\n",
       "      <td>Personal Travel</td>\n",
       "      <td>Eco</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>satisfied</td>\n",
       "      <td>Male</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>47</td>\n",
       "      <td>Personal Travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>2464</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>310</td>\n",
       "      <td>305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>satisfied</td>\n",
       "      <td>Female</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>15</td>\n",
       "      <td>Personal Travel</td>\n",
       "      <td>Eco</td>\n",
       "      <td>2138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>satisfied</td>\n",
       "      <td>Female</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>60</td>\n",
       "      <td>Personal Travel</td>\n",
       "      <td>Eco</td>\n",
       "      <td>623</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>satisfied</td>\n",
       "      <td>Female</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>70</td>\n",
       "      <td>Personal Travel</td>\n",
       "      <td>Eco</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  satisfaction  Gender   Customer Type  Age   Type of Travel     Class  \\\n",
       "0    satisfied  Female  Loyal Customer   65  Personal Travel       Eco   \n",
       "1    satisfied    Male  Loyal Customer   47  Personal Travel  Business   \n",
       "2    satisfied  Female  Loyal Customer   15  Personal Travel       Eco   \n",
       "3    satisfied  Female  Loyal Customer   60  Personal Travel       Eco   \n",
       "4    satisfied  Female  Loyal Customer   70  Personal Travel       Eco   \n",
       "\n",
       "   Flight Distance  Seat comfort  Departure/Arrival time convenient  \\\n",
       "0              265             0                                  0   \n",
       "1             2464             0                                  0   \n",
       "2             2138             0                                  0   \n",
       "3              623             0                                  0   \n",
       "4              354             0                                  0   \n",
       "\n",
       "   Food and drink  ...  Online support  Ease of Online booking  \\\n",
       "0               0  ...               2                       3   \n",
       "1               0  ...               2                       3   \n",
       "2               0  ...               2                       2   \n",
       "3               0  ...               3                       1   \n",
       "4               0  ...               4                       2   \n",
       "\n",
       "   On-board service  Leg room service  Baggage handling  Checkin service  \\\n",
       "0                 3                 0                 3                5   \n",
       "1                 4                 4                 4                2   \n",
       "2                 3                 3                 4                4   \n",
       "3                 1                 0                 1                4   \n",
       "4                 2                 0                 2                4   \n",
       "\n",
       "   Cleanliness  Online boarding  Departure Delay in Minutes  \\\n",
       "0            3                2                           0   \n",
       "1            3                2                         310   \n",
       "2            4                2                           0   \n",
       "3            1                3                           0   \n",
       "4            2                5                           0   \n",
       "\n",
       "   Arrival Delay in Minutes  \n",
       "0                       0.0  \n",
       "1                     305.0  \n",
       "2                       0.0  \n",
       "3                       0.0  \n",
       "4                       0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임의 처음 5개 행의 데이터 출력하기\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37da9f36-9fc4-4378-9616-d899d861df5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1872631259.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    데이터의 통계량을 확인합니다.\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "데이터의 통계량을 확인합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e3395-5f57-4673-b890-ae18d0e53df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d581e-39c2-4b45-9ac2-ebc469b179f2",
   "metadata": {},
   "source": [
    "각 칼럼별로 결측치가 있는지 확인합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66288d35-30ff-48ab-b092-85b2bc3e33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1eef9c-83d0-4a1c-a9f3-acb21a89da1b",
   "metadata": {},
   "source": [
    "### 2) 데이터 전처리하기\n",
    "데이터를 확인한 결과, 도착 지연 시간(​Arrival Delay in Minutes) 칼럼에 Null 값이 393개 있으며, 텍스트로 구분되는 범주형 칼럼이 다수 있습니다. 신경망 모델의 입력 데이터에는 결측치가 없어야 하고, 기본적으로 수치형 데이털르 사용합니다. 그러므로 범주형 데이터를 수치형 데이터로 변환하는 인코딩 작업을 하며, 수치형 데이터도 모델 성능을 높이기 위해 데이터 스케일링 등의 변환 작업이 요구됩니다. \n",
    "#### (1) 결측치 처리하기\n",
    "사이킷런 SimpleImputer 객체를 사용해서 도착 지연 시간(​Arrival Delay in Minutes) 칼럼에 있는 결측치를 평균값으로 치환합니다. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a43bee7-5392-46ce-9cdb-440c904a255d",
   "metadata": {},
   "source": [
    "#265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d560c-f1cb-42a0-86f1-9b6f863cf59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4329c38-039b-4011-9dd1-5a64f219343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleImputer 객체로 결측치 대체하기\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "df[\"Arrival Delay in Minutes\"] = mean_imputer.fit_transform(df[[\"Arrival Delay in Minutes\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a00bb-bc36-479c-bc6e-485818b500ee",
   "metadata": {},
   "source": [
    "#### (2) 데이터 인코딩하기\n",
    "Object 칼럼의 유형을 String 유형으로 변경합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef324e-f7a5-4953-ac48-60a6c0e2b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object 칼럼 유형을 string 유형으로 변경하기\n",
    "cols = ['satisfaction', 'Gender', 'Customer Type', 'Type of Travel', 'Class']\n",
    "df[cols] = df[cols].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7195f7e1-0789-41bb-93b6-0efc67e5b461",
   "metadata": {},
   "source": [
    "범주형 데이터를 수치값으로 변경합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d58e7db-5066-404f-a11b-47c1f821f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 데이터를 수치값으로 변경하기\n",
    "df['satisfaction'].replace(['dissatisfied','satisfied'], [0,1], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5e0dd-ee2e-4f91-86e6-a5faf2402819",
   "metadata": {},
   "source": [
    "좌석 등급(Class) 칼럼은 순서를 고려해 정수 1~N으로 순서형 인코딩(Ordinal Encoding)을 합니다. ‘Eco’, ‘Eco Plus’, ‘Business’ 값이 0, 1, 2로 변환됩니다. 순서형 인코딩의 경우 판다스의 Categorial 함수를 활용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdddff2-c67f-487d-852b-13388d1e9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순서형 인코딩(Ordinal Encoding)하기\n",
    "categories = pd.Categorical(\n",
    "    df['Class'], \n",
    "    categories= ['Eco', 'Eco Plus', 'Business'], \n",
    "    ordered=True)\n",
    "labels, unique = pd.factorize(categories, sort=True)\n",
    "df['Class'] = labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e456ffb1-9d5f-4d91-ae1d-579d3088be65",
   "metadata": {},
   "source": [
    "#266"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4befca3d-6b0b-45f9-8003-8c6aa3c3ea09",
   "metadata": {},
   "source": [
    "범주형 데이터인 성별(Gender), 고객 유형(Customer Type), 여행 유형(Type of Travel) 칼럼은 원핫 인코딩을 적용합니다. 원핫 인코딩은 0과 1의 벡터로만 표현하는 기법입니다. 범주(Gategory)의 수만큼 벡터의 수가 생성되므로, 각 범주가 새로운 특성이 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218c30df-9b91-40db-821b-baca30872590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫 인코딩(One Hot Encoding)하기\n",
    "cat_cols = ['Gender','Customer Type','Type of Travel']\n",
    "df = pd.get_dummies(df, columns=cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8130c9c9-ea8a-4055-b1d2-0240b1bf21c4",
   "metadata": {},
   "source": [
    "데이터 전처리 결과를 확인합니다. 모든 데이터 유형이 수치형 데이터로 변경되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f36c8-e760-44a6-821f-01809716a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84576564-747c-48b3-8bd0-f986de277981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53bba30-f7fe-475a-b8a8-86cb1031a225",
   "metadata": {},
   "source": [
    "#### (3) 데이터세트 분리하기\n",
    "데이터세트를 입력(X)과 레이블(y)로 분리하고, 훈련 데이터세트(train dataset)와 검증 데이터세트(validation dataset)로 분리합니다. 데이터세트는 test_size에 지정한 비율로 분리됩니다. test_size를 0.2로 지정했으므로, 훈련 데이터는 80%, 검증 데이터는 20%의 비율로 데이터세트가 분리됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3b740-e563-4a31-bbf9-35ffebf0e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터셋을 입력(X)과 레이블(y)로 분리하기\n",
    "X = df.drop(['satisfaction'], axis=1)\n",
    "y = df['satisfaction'].reset_index(drop=True)\n",
    "\n",
    "# 데이터셋을 훈련 데이터와 검증 데이터로 분리하기\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y)\n",
    "\n",
    "print(f'훈련 데이터셋 크기 : X_train {X_train.shape}, y_train {y_train.shape}')\n",
    "print(f'검증 데이터셋 크기 : X_val {X_val.shape}, y_val {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da8c28-4795-48ea-bde7-64b922ffd280",
   "metadata": {},
   "source": [
    "#### (4) 데이터 스케일링하기\n",
    "특성별로 데이터의 스케일이 다르면 딥러닝이 잘 동작하지 않을 수 있습니다. 따라서 데이터 스케일링 작업을 통해 모든 특성의 범위(또는 분포)를 유사하게 만들어줘야 합니다. 사이킷런 MinMaxScaler 객체로 데이터의 최솟값과 최댓값을 이용하여 데이터를 특정 범위(주로 0~1 사이)로 스케일링하여 특성을 정규화합니다. 사이킷런의 StandardScaler 객체도 스케일링에 많이 사용합니다. StandardSaler는 특성들의 평균을 0, 분산을 1로 스케일링하여 정규분포로 만드는 표준화를 합니다. <br>\n",
    "이번 실습에서는 MinMaxScaler 객체를 생성하여 fit 메소드로 학습시킨 후 transform 메소드를 사용하여 데이터를 변환합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7151743-ec1a-40df-8e4b-5c630893287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 데이터 정규화하기\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271d92c-262d-4e62-8ffd-97485a4e5748",
   "metadata": {},
   "source": [
    "### 3) 심층신경망 모델 생성하기\n",
    "입력 데이터는 25개, 은닉층은 여러 개, 출력은 1개인 이진 분류를 위한 심층신경망(DNN, Deep Neural Network) 모델을 구성합니다. 은닉층의 활성화 함수는 ‘relu’를 사용하고, 마지막 층의 활성화 함수는 출력이 1개인 이진 분류 모델이므로 ‘sigmoid’를 사용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a02bac-efdb-4838-a325-f20f261781b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import random\n",
    "\n",
    "# 모델 시드 고정하기\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Keras의 Sequential 객체로 딥러닝 모델 구성하기\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=42) #모델 시드 고정하기\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(25,),kernel_initializer=initializer))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33513d57-9e25-41cd-90a1-874c4e874c7a",
   "metadata": {},
   "source": [
    "모델 구조와 파라미터의 개수를 확인해 보면, 모델은 총 8개의 층으로 구성되었고 학습해야 할 총 파라미터 개수(Total params)는 2만 1,633개입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffbb80-2c7a-4147-b837-68345f428312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조 및 파라미터 정보 확인하기\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4543d0-304c-44cb-9dfc-7dd8766409cd",
   "metadata": {},
   "source": [
    "#### 4) 모델 컴파일하기\n",
    "optimizer는 adam, 모델 성능 평가 메트릭으로는 정확도를, 이진분류 모델이므로 손실함수는 binary_crossentropy를 사용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecbc6ec-c351-43d1-8135-cdb41df28b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법 설정하기\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d445e0a-ab8c-42db-815e-ebe5356ab332",
   "metadata": {},
   "source": [
    "#271"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a149c7d-8826-4ddd-a86d-4d5616a86353",
   "metadata": {},
   "source": [
    "#### 5) 모델 학습하기\n",
    "모델 학습 시 너무 많은 에포크(epoch)는 과대적합을 발생시킵니다. 이를 방지하기 위한 방법으로 조기 종료(Early Stopping)가 있습니다. 조기 종료는 검증 데이터세트에서 성능이 더 이상 증가하지 않으면 모델 학습을 중단하는 방법입니다. EarlyStopping 객체를 생성하여 model.fit 함수의 callback 매개변수에 넣어주면 조기 종료가 적용됩니다. EarlyStopping의 주요 인자는 다음과 같습니다. \n",
    "- monitor：학습 조기 종료를 위한 성능 모니터링 도구로, val_loss나 val_accuracy가 주로 사용되며 기본값은 val_loss입니다. \n",
    "- min_delta：개선되고 있다고 판단하기 위한 최소 변화량입니다. \n",
    "- patience：성능 향상을 몇 번의 에포크 동안 기다릴지 설정하며 기본값은 0입니다.\n",
    "- verbose：얼마나 자세하게 정보를 표출할 것인가를 지정하며, 가능한 값은 0, 1, 2입니다. \n",
    "- mode：성능 모니터링 도구의 개선 판단기준으로, monitor  설정 항목이 val_loss이면 min을 설정하고, val_accuracy이면 max를 설정해야 합니다. 기본값은 auto로 monitor 설정된 항목에 따른 판단 기준을 자동으로 지정합니다. \n",
    "- restore_best_weights：관할 항목의 가장 좋은 값을 가지는 에포크의 모델 가중치 복원(restore) 여부입니다. 기본값은 False로, 학습 에포크의 마지막 가중치를 보존합니다. \n",
    "\n",
    "예제 코드에서 validation loss가 10번 이상 개선되지 않으면 학습을 중단하고, 가장 성능이 좋았을 때의 가중치를 사용합니다. epochs는 100으로 설정했고, 훈련 과정의 loss, accuracy를 history에 저장합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61c6bb-0887-43e1-bce1-2465dff009c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습하기\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=128,\n",
    "          verbose=1, validation_data=(X_val, y_val), callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67062ae8-332e-487e-afcd-6f7677a414e6",
   "metadata": {},
   "source": [
    "### 6) 모델 훈련 과정 시각화하기\n",
    "신경망 모델의 훈련에 사용되는 fit()메소드는 history 객체를 반환합니다. history.history 속성은 모델의 훈련 과정에서 에포크(epoch)에 따른 정확도(accuracy)와 같은 성능 지표와 손실값을 기록합니다. 그리고 검증 지표와 손실값도 기록합니다.<br> \n",
    "예시 코드는 딥러닝 모델의 훈련 과정에서 기록된 정확도(accuracy), 검증 정확도(val_accuracy), 손실(loss), 검증 손실(val_loss)을 그래프로 출력하여 시각화합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc740c-061b-44a7-a4ec-5f2e23e8b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6627c-8a36-4c65-8a93-5cef4554ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 훈련 과정 정확도(accuracy) 시각화하기\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# 훈련 과정 손실(loss) 시각화하기\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ea39b-77ba-45f2-aea6-a0efa6f59a2c",
   "metadata": {},
   "source": [
    "## 2. 기계 학습(ML, Machine Learning)\n",
    "### 1) 정의: \n",
    "기계 학습은 데이터로부터 패턴을 학습하여 새로운 데이터에 대해 예측하거나 의사결정을 내리는 알고리즘을 의미합니다.<br>\n",
    "사람이 일일이 프로그램을 작성하지 않아도, 컴퓨터가 데이터에서 스스로 학습하여 성능을 개선하는 것이 목표입니다. \n",
    "\n",
    "**학습(Training)이란,**<br>\n",
    "- 데이터를 기반으로 모델이 규칙을 찾아내고, 이를 통해 새로운 데이터를 예측할 수 있는 능력을 갖추는 과정을 의미합니다. \n",
    "- 이 과정에서 모델은 주어진 데이터를 바탕으로 패턴을 학습하고, 이를 통해 미래의 데이터를 예측하거나 분류하는 능력을 갖추게 됩니다. \n",
    "### 2) 사례: \n",
    "이메일 스팸 필터링, 사용자 맞춤형 추천 시스템(Netflix나 YouTube의 콘텐츠 추천), 질병 진단 예측 모델 등이 기계 학습의 활용 예입니다. \n",
    "### 3) 특징:\n",
    "기계 학습은 지도 학습(Supervised Learning), 비지도 학습(Unsupervised Learning), 강화 학습(Reinforcement Learning) 등의 다양한 학습 방법을 포함합니다. 데이터를 이용해 모델을 학습시키고, 학습된 모델은 새로운 데이터를 처리하고 예측하는 데 사용됩니다. 기계 학습은 주로 통계적 모델과 알고리즘을 기반으로 패턴을 인식하고, 이를 바탕으로 의사 결정을 내립니다.\n",
    "\n",
    "![이미지 설명](./001-04.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b25be69-23ff-4f4a-8f62-fe5b36b2b079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8500e628-08d9-46ab-98cc-5bfeed4d9bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
